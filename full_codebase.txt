
==================================================
FILE PATH: .gitignore
==================================================

# Byte-compiled / optimized / DLL files
__pycache__/
*.py[codz]
*$py.class

# C extensions
*.so

# Distribution / packaging
.Python
build/
develop-eggs/
dist/
downloads/
eggs/
.eggs/
lib/
lib64/
parts/
sdist/
var/
wheels/
share/python-wheels/
*.egg-info/
.installed.cfg
*.egg
MANIFEST

# PyInstaller
#  Usually these files are written by a python script from a template
#  before PyInstaller builds the exe, so as to inject date/other infos into it.
*.manifest
*.spec

# Installer logs
pip-log.txt
pip-delete-this-directory.txt

# Unit test / coverage reports
htmlcov/
.tox/
.nox/
.coverage
.coverage.*
.cache
nosetests.xml
coverage.xml
*.cover
*.py.cover
.hypothesis/
.pytest_cache/
cover/

# Translations
*.mo
*.pot

# Django stuff:
*.log
local_settings.py
db.sqlite3
db.sqlite3-journal

# Flask stuff:
instance/
.webassets-cache

# Scrapy stuff:
.scrapy

# Sphinx documentation
docs/_build/

# PyBuilder
.pybuilder/
target/

# Jupyter Notebook
.ipynb_checkpoints

# IPython
profile_default/
ipython_config.py

# pyenv
#   For a library or package, you might want to ignore these files since the code is
#   intended to run in multiple environments; otherwise, check them in:
# .python-version

# pipenv
#   According to pypa/pipenv#598, it is recommended to include Pipfile.lock in version control.
#   However, in case of collaboration, if having platform-specific dependencies or dependencies
#   having no cross-platform support, pipenv may install dependencies that don't work, or not
#   install all needed dependencies.
#Pipfile.lock

# UV
#   Similar to Pipfile.lock, it is generally recommended to include uv.lock in version control.
#   This is especially recommended for binary packages to ensure reproducibility, and is more
#   commonly ignored for libraries.
#uv.lock

# poetry
#   Similar to Pipfile.lock, it is generally recommended to include poetry.lock in version control.
#   This is especially recommended for binary packages to ensure reproducibility, and is more
#   commonly ignored for libraries.
#   https://python-poetry.org/docs/basic-usage/#commit-your-poetrylock-file-to-version-control
#poetry.lock
#poetry.toml

# pdm
#   Similar to Pipfile.lock, it is generally recommended to include pdm.lock in version control.
#   pdm recommends including project-wide configuration in pdm.toml, but excluding .pdm-python.
#   https://pdm-project.org/en/latest/usage/project/#working-with-version-control
#pdm.lock
#pdm.toml
.pdm-python
.pdm-build/

# pixi
#   Similar to Pipfile.lock, it is generally recommended to include pixi.lock in version control.
#pixi.lock
#   Pixi creates a virtual environment in the .pixi directory, just like venv module creates one
#   in the .venv directory. It is recommended not to include this directory in version control.
.pixi

# PEP 582; used by e.g. github.com/David-OConnor/pyflow and github.com/pdm-project/pdm
__pypackages__/

# Celery stuff
celerybeat-schedule
celerybeat.pid

# SageMath parsed files
*.sage.py

# Environments
.env
.envrc
.venv
env/
venv/
ENV/
env.bak/
venv.bak/

# Spyder project settings
.spyderproject
.spyproject

# Rope project settings
.ropeproject

# mkdocs documentation
/site

# mypy
.mypy_cache/
.dmypy.json
dmypy.json

# Pyre type checker
.pyre/

# pytype static type analyzer
.pytype/

# Cython debug symbols
cython_debug/

# PyCharm
#  JetBrains specific template is maintained in a separate JetBrains.gitignore that can
#  be found at https://github.com/github/gitignore/blob/main/Global/JetBrains.gitignore
#  and can be added to the global gitignore or merged into this file.  For a more nuclear
#  option (not recommended) you can uncomment the following to ignore the entire idea folder.
#.idea/

# Abstra
# Abstra is an AI-powered process automation framework.
# Ignore directories containing user credentials, local state, and settings.
# Learn more at https://abstra.io/docs
.abstra/

# Visual Studio Code
#  Visual Studio Code specific template is maintained in a separate VisualStudioCode.gitignore 
#  that can be found at https://github.com/github/gitignore/blob/main/Global/VisualStudioCode.gitignore
#  and can be added to the global gitignore or merged into this file. However, if you prefer, 
#  you could uncomment the following to ignore the entire vscode folder
# .vscode/

# Ruff stuff:
.ruff_cache/

# PyPI configuration file
.pypirc

# Cursor
#  Cursor is an AI-powered code editor. `.cursorignore` specifies files/directories to
#  exclude from AI features like autocomplete and code analysis. Recommended for sensitive data
#  refer to https://docs.cursor.com/context/ignore-files
.cursorignore
.cursorindexingignore

# Marimo
marimo/_static/
marimo/_lsp/
__marimo__/


==================================================
FILE PATH: LICENSE
==================================================

MIT License

Copyright (c) 2025 qognus

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.


==================================================
FILE PATH: README.md
==================================================

# Qognus Demo Platform  
*A fully synthetic, production-style environment for applied AI/ML demos*

The **Qognus Demo Platform** is an end-to-end demonstration and experimentation environment built around a fictional hybrid enterprise, **ApexGrid Systems**. It provides realistic synthetic datasets, embedding & clustering pipelines, model-health evaluation, agent workflows, and an interactive WebGL visualization layer.

This project enables safe, compliance-free demonstrations of real enterprise AI concepts across:
- NLP and text classification  
- embeddings & vector search  
- anomaly detection  
- multi-modal clustering  
- LLM-based ticket generation  
- agentic triage workflows  
- forecasting & operational analytics  

It is suitable for:
- client proofs of concept  
- consulting engagements  
- internal demos and playbooks  
- technical workshops and conference talks  
- websites and portfolios  
- research prototypes  

---

## ğŸ”§ Key Components

### 1. Synthetic Enterprise Environment â€” ApexGrid Systems

ApexGrid is a fictional mid-to-large hybrid enterprise operating across SaaS, energy utilities, industrial IoT, and cybersecurity.

Products:

- **HelioCloud** â€” SaaS observability & APM  
- **GridSense** â€” energy/utility IoT monitoring  
- **LineaOps** â€” manufacturing & robotics telemetry  
- **VaultShield** â€” identity & security analytics  

The repository includes documentation describing:

- the product suite  
- support ticket taxonomy  
- operational challenges  
- customer tiers and regions  
- realistic metadata schema  

This serves as the canonical foundation for all synthetic data and ML pipelines.

---

### 2. Synthetic Data Generation (Local LLM via Ollama)

The `/synthetic` module uses local LLMs (e.g. Qwen, LLaMA via **Ollama**) to generate thousands of fully synthetic support tickets, each including:

- product, category, subcategory  
- severity and customer tier  
- environment and region  
- timestamp  
- summary and full description  
- optional LLM-generated topics  

All data is:

- fictional and non-identifying  
- free from real PII or company names  
- consistent with the ApexGrid taxonomy  

Main pieces:

- `synthetic/generate_tickets_ollama.py`  
- prompt templates per product  
- sample batches for inspection and testing  

---

### 3. Embedding & Clustering Pipeline

Located in `/models/embed` and `/models/cluster`, this pipeline transforms synthetic text into high-dimensional vectors using local embedding models such as:

- `mxbai-embed-large`  
- `nomic-embed-text`  

It then applies:

- UMAP / t-SNE / PaCMAP for 2D/3D manifolds  
- HDBSCAN or similar algorithms for semantic clustering  
- duplicate/near-duplicate detection  
- cluster labeling and keyword extraction  

Outputs include:

- `ticket_points.js` for WebGL visualizations  
- `embeddings.npy`  
- `cluster_labels.npy`  

---

### 4. Model Evaluation / Health Metrics

The `/models/eval` module provides metrics to characterize the embedding and clustering quality, such as:

- silhouette scores  
- cluster cohesion and separation  
- cluster stability  
- topic entropy  
- severity and category distributions  
- drift over time  
- precision@k for duplicate retrieval  
- cluster purity (when ground-truth labels are used)  

Aggregated outputs are written to:

- `model_health.json`  
- JavaScript payloads consumed by the web UI  

---

### 5. Interactive Web Visualization (WebGL + Charts)

The `/web` folder contains a standalone visualization layer built with:

- **Three.js** for 3D point clouds  
- **TailwindCSS** for styling  
- **Chart.js** for charts  
- lightweight **Vue.js** state management  

Features:

- 3D embedding view of support tickets  
- coloring by category, product, or severity  
- hover tooltips for representative samples  
- charts for category distribution, severity mix, and volume over time  
- model-health and cluster-quality summaries  

This layer can be embedded into websites or used standalone for live demos.

---

## ğŸ“ Repository Structure

```text
qognus-demo-platform/
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/
â”‚   â”œâ”€â”€ processed/
â”‚   â””â”€â”€ taxonomy/
â”‚
â”œâ”€â”€ synthetic/
â”‚   â”œâ”€â”€ generate_tickets_ollama.py
â”‚   â”œâ”€â”€ prompts/
â”‚   â””â”€â”€ examples/
â”‚
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ embed/
â”‚   â”œâ”€â”€ cluster/
â”‚   â””â”€â”€ eval/
â”‚
â”œâ”€â”€ web/
â”‚   â”œâ”€â”€ index.html
â”‚   â”œâ”€â”€ js/
â”‚   â”œâ”€â”€ css/
â”‚   â””â”€â”€ assets/
â”‚
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ apexgrid_overview.md
â”‚   â”œâ”€â”€ product_suite.md
â”‚   â”œâ”€â”€ support_taxonomy.md
â”‚   â”œâ”€â”€ ml_pipeline_design.md
â”‚   â””â”€â”€ architecture.png
â”‚
â””â”€â”€ notebooks/
    â”œâ”€â”€ 01_generate_synthetic.ipynb
    â”œâ”€â”€ 02_embeddings.ipynb
    â”œâ”€â”€ 03_clustering.ipynb
    â”œâ”€â”€ 04_model_health.ipynb
    â””â”€â”€ 05_visualization_prototyping.ipynb
````

---

## ğŸš€ Getting Started

### 1. Install dependencies

#### Python

```bash
pip install -r requirements.txt
```

#### Ollama

Install from: [https://ollama.com/download](https://ollama.com/download)

Pull the required models, for example:

```bash
ollama pull qwen3:8b
ollama pull mxbai-embed-large
```

---

### 2. Generate synthetic tickets

```bash
python synthetic/generate_tickets_ollama.py
```

This will create a JSONL file of synthetic support tickets under `data/raw/` (or similar, depending on configuration).

---

### 3. Compute embeddings

```bash
python models/embed/compute_embeddings.py
```

Embeddings are written to `data/processed/embeddings.npy` and corresponding metadata structures.

---

### 4. Cluster & evaluate

```bash
python models/cluster/cluster_umap_hdbscan.py
python models/eval/embedding_health.py
```

Cluster assignments, manifold coordinates, and evaluation metrics are stored in `data/processed/` and web-ready JS files in `web/data/`.

---

### 5. Launch the web visualization

From the `web` directory:

```bash
cd web
python -m http.server 3000
```

Then open:

```text
http://localhost:3000
```

in a browser to explore the 3D embedding, charts, and model-health summaries.

---

## ğŸ§  Intended Use

The Qognus Demo Platform is designed to:

* demonstrate applied AI/ML techniques without real-world data
* support repeatable, transparent POCs with a consistent synthetic enterprise
* act as a sandbox for experimenting with embeddings, clustering, and agent workflows
* provide visually compelling assets for presentations, workshops, and websites

Because all data is synthetic and the enterprise is fictional, it can be safely shared, extended, and adapted.

---

## ğŸ›¡ License

MIT â€“ see `LICENSE` for details.
Suitable for demos, workshops, educational use, and research.


==================================================
FILE PATH: requirements.txt
==================================================

langchain
langchain-core
langchain-community
langchain-ollama
pydantic
pandas
numpy
requests
tqdm
umap-learn
hdbscan
scikit-learn
pyarrow


==================================================
FILE PATH: docs\apexgrid_overview.md
==================================================

# **ApexGrid Systems â€” Enterprise Overview**

*Fictional Hybrid Enterprise for Applied AI/ML Demonstrations*

---

## **1. Introduction**

**ApexGrid Systems** is a fictional mid-to-large hybrid enterprise designed to provide a realistic, safe, and repeatable environment for demonstrating applied AI and machine learning workflows. The company operates across SaaS observability, energy utilities monitoring, industrial IoT, and cybersecurity.

All products, data, and scenarios are entirely synthetic. ApexGrid exists solely to support:

* applied AI/ML proofs of concept
* embeddings & clustering demos
* anomaly detection pipelines
* synthetic support ticket generation
* forecasting and operational analytics
* LLM-powered triage and agentic workflows
* WebGL visualization experiences
* research and workshop examples

---

## **2. Company Profile**

**Name:** ApexGrid Systems
**Founded:** 2014
**Headquarters:** Denver, Colorado
**Regional Offices:** Toronto, Berlin, Singapore
**Employees:** ~1,800 (fictional)
**Annual Revenue:** ~$420M
**Deployment Footprint:** Hybrid cloud (AWS + GCP) + on-prem edge clusters
**Target Markets:** Energy utilities, manufacturing, SaaS platforms, enterprise security, logistics, robotics

ApexGrid operates at the intersection of physical operations and digital infrastructure. Its platform products unify telemetry, events, alerts, and identity signals into a single operational view.

This combination makes ApexGrid an ideal â€œsynthetic enterpriseâ€ for generating rich, diverse datasets covering:

* textual support cases
* IoT telemetry
* operational metrics
* cybersecurity alerts
* system events

---

## **3. Product Suite**

ApexGrid has four major platform offerings. Together they produce a realistic distribution of support issues, metadata, and natural cluster patterns for embedding spaces.

---

### **3.1 HelioCloud â€” SaaS Observability & APM**

A SaaS-based observability platform focused on:

* metrics, logs, and traces
* SLO dashboards
* alert routing and deduplication
* microservice dependency maps
* incident signatures using ML

**Typical Support Themes:**

* delayed alerts
* missing metrics in dashboards
* ingestion pipeline failures
* flaky agents
* high cardinality issues
* API quota enforcement

---

### **3.2 GridSense â€” Energy & Utilities IoT Monitoring**

A system for monitoring distributed grid assets and substations.
Supports:

* voltage & frequency anomaly detection
* SCADA integration
* predictive maintenance
* edge gateway management

**Typical Support Themes:**

* telemetry dropouts
* SCADA desynchronization
* edge gateway disconnects
* voltage anomaly false positives
* firmware version mismatches

---

### **3.3 LineaOps â€” Industrial & Manufacturing Cloud**

Factory-floor ingestion and robotics telemetry platform:

* PLC (Programmable Logic Controller) integration
* robotics fleet health
* conveyor & throughput metrics
* predictive downtime analysis

**Typical Support Themes:**

* PLC driver failures
* robot arm offline events
* rising reject rates
* throughput anomalies
* firmware deployment issues

---

### **3.4 VaultShield â€” Identity & Security Analytics**

A security analytics platform offering:

* identity anomaly detection
* MFA & SSO monitoring
* SOC dashboards
* threat ingestion and classification
* SIEM (Splunk/Azure/Elastic) integration

**Typical Support Themes:**

* MFA drift or sync failures
* brute-force false positives
* failed SSO federations
* SIEM connector issues
* noisy threat signatures

---

## **4. Customer Segments**

To create realistic data for embeddings and analytics, ApexGrid defines three fictional customer tiers.

### **4.1 Enterprise Tier**

Large utilities, global manufacturers, Fortune 500 SaaS platforms.

### **4.2 Midmarket Tier**

Regional factories, municipal utilities, logistics firms.

### **4.3 Startup Tier**

Narrow vertical SaaS, robotics startups, energy analytics vendors.

These tiers influence severity, region, SLA expectations, and metadata distributions.

---

## **5. Operational Environment**

ApexGrid products operate across:

* **Environments:** production, staging, sandbox
* **Regions:** `us-west-2`, `us-east-1`, `eu-central-1`, `ap-southeast-1`
* **Channels:** email, web portal, Slack, phone transcript

This spread produces a wide range of realistic synthetic ticket metadata.

---

## **6. Support Ticket Taxonomy**

ApexGridâ€™s synthetic support system uses a structured taxonomy that maps cleanly to semantic embedding clusters.

### **Primary Categories**

* authentication
* authorization
* billing
* latency
* integration
* data_quality
* api_errors
* telemetry_drop
* security_alerts
* observability
* deployment_failures
* firmware
* scaling
* dashboard_issues

### **Subcategories (Examples)**

* authentication â†’ `MFA_failure`, `SSO_drift`
* data_quality â†’ `missing_values`, `drift_detected`, `outliers`
* telemetry_drop â†’ `edge_offline`, `sensor_unreachable`
* api_errors â†’ `429_rate_limit`, `500_internal`
* integration â†’ `SCADA_protocol_error`, `PLC_driver_fault`, `SIEM_misconfiguration`
* security_alerts â†’ `bruteforce_detected`, `anomalous_login`, `malicious_IP`

This taxonomy supports clean ML labeling, clustering, and explainability.

---

## **7. Synthetic Data Strategy**

All ApexGrid data is fully synthetic, generated via:

* local LLMs using **Ollama**
* structured prompts
* seeded metadata (severity, product, category, timestamp)
* consistent domain vocabulary
* realistic descriptions (3â€“8 sentences)

The synthetic dataset includes:

* ~10,000 support tickets
* realistic timestamp sequences
* category & subcategory patterns
* severity distributions
* customer tier metadata
* environment + region tags
* optional topic lists

The design encourages natural clusters in embedding spaces.

---

## **8. ML/AI Use Cases Demonstrated**

ApexGrid is intentionally structured to support a wide range of ML applications.

### **8.1 Embeddings & Semantic Search**

* support ticket clustering
* duplicate detection
* representative ticket identification
* emerging issue detection

### **8.2 Topic Modeling & Clustering**

* UMAP/TSNE/PACMAP embeddings
* HDBSCAN clusters
* keyword extraction
* cluster quality metrics

### **8.3 Predictive Analytics**

* support volume forecasting
* severity drift
* category trend analysis
* anomaly classification

### **8.4 LLM Copilot Workflows**

* ticket triage
* summarization
* routing suggestions
* auto-categorization
* escalation decisioning

### **8.5 Multi-Modal Demonstrations**

Future datasets may include:

* IoT sensor traces
* cybersecurity alert logs
* application latency curves
* change management events

---

## **9. Architecture (Conceptual)**

ApexGridâ€™s fictional architecture supports multi-product pipelines that mirror real enterprise complexity.

```
[IoT Sensors / Agents / SCADA / Identity Signals]
                     â”‚
              [Edge Gateways]
                     â”‚
            [Event & Metric Ingestion]
                     â”‚
         [HelioCloud / GridSense / LineaOps / VaultShield]
                     â”‚
                [ApexGrid Ops Portal]
                     â”‚
              [Support Ticket System]
                     â”‚
             [Qognus AI/ML Pipeline]
    (embeddings â†’ clustering â†’ triage â†’ insights)
                     â”‚
            [WebGL & Analytics Dashboards]
```

This end-to-end structure is perfect for demonstrations.

---

## **10. Purpose of ApexGrid Within the Qognus Demo Platform**

The ApexGrid environment exists to enable:

* safe, compliant synthetic data
* consistent demos across industries
* reusable embeddings for multiple POCs
* visually compelling 3D embeddings
* realistic ML evaluation metrics
* agent workflow experimentation
* replicable client-facing scenarios

It centralizes all synthetic demonstrations behind a single cohesive fictional organization.

---

## **11. Extensibility**

ApexGrid is intentionally modular.

Future additions may include:

* synthetic IoT telemetry for predictive maintenance
* synthetic identity threat logs
* synthetic application performance events
* synthetic billing records
* synthetic infrastructure topology graphs
* synthetic conversation transcripts for call-center LLM demos

All can be integrated seamlessly with the current taxonomy and pipeline.

---

## **12. Summary**

ApexGrid Systems is a deliberately versatile hybrid enterprise designed as the core of the **Qognus Demo Platform**.
It provides a realistic and richly-structured environment that supports:

* LLM data generation
* embeddings
* clustering
* anomaly detection
* ML evaluation
* agent workflows
* visualization demos

All while remaining simple, modular, and entirely synthetic.

ApexGrid is the fictional foundation for any future POC or demonstration involving applied AI.

==================================================
FILE PATH: docs\architecture.md
==================================================

# **Qognus Demo Platform â€” Architecture**

*ApexGrid Systems Synthetic Enterprise & ML Visualization Stack*

---

## 1. Purpose

This document describes the **system architecture** of the Qognus Demo Platform, which provides a fully synthetic environment for experimenting with and demonstrating applied AI/ML techniques on the fictional hybrid enterprise **ApexGrid Systems**.

The architecture covers:

* synthetic data generation
* ML/embedding pipeline
* evaluation & metrics
* static web visualization
* optional agentic workflows

All components are designed to run on a single developer machine or small server, with no external data dependencies and no real customer information.

---

## 2. High-Level System Overview

At a high level, the system has four main layers:

1. **Synthetic Data Layer**

   * Uses local LLMs (Ollama) to generate ApexGrid support tickets and (optionally) other signals.
2. **ML & Analytics Layer**

   * Computes embeddings, projections, clusters, and model-health metrics.
3. **Artifact Layer**

   * Converts ML outputs into web-friendly JSON/JS payloads and charts.
4. **Visualization & Interaction Layer**

   * Renders a WebGL 3D embedding, charts, and dashboards in a browser.

### 2.1 High-Level Diagram (ASCII)

```text
           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
           â”‚   Synthetic Data Layer   â”‚
           â”‚  (LLMs via Ollama)       â”‚
           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚   JSONL tickets
                      â–¼
           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
           â”‚   ML & Analytics Layer   â”‚
           â”‚ embeddings, clustering,  â”‚
           â”‚ evaluation, metrics      â”‚
           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚  JS/JSON artifacts
                      â–¼
           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
           â”‚   Artifact Layer         â”‚
           â”‚ web-ready JS payloads    â”‚
           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚  static files
                      â–¼
           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
           â”‚   Visualization Layer    â”‚
           â”‚ WebGL + charts (browser) â”‚
           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 3. Context: ApexGrid & Qognus Demo Platform

The architecture is built around:

* **ApexGrid Systems**: a fictional hybrid enterprise with four products:

  * HelioCloud (SaaS observability)
  * GridSense (energy/utility IoT monitoring)
  * LineaOps (manufacturing & robotics)
  * VaultShield (identity & security analytics)

* **Qognus Demo Platform**: a set of synthetic data generators, ML pipelines, and visualizations that:

  * showcase applied AI workflows
  * demonstrate embeddings & clustering
  * remain fully local and safe for public use

---

## 4. Component Architecture

### 4.1 Synthetic Data Layer

**Primary Role:** Create realistic, taxonomy-aligned ApexGrid support tickets using local LLMs.

**Key Components:**

* `synthetic/generate_tickets_ollama.py`

  * Reads a metadata matrix (product, category, severity, etc.)
  * Builds structured prompts
  * Calls Ollamaâ€™s `/api/chat` endpoint
  * Receives JSON tickets and validates them

* `docs/data_generation_design.md`

  * Defines prompting strategy, constraints, and distributions

**Primary Output:**

* `data/raw/apexgrid_tickets.jsonl`

  * One JSON object per line
  * Matches the schema in `support_taxonomy.md`

### 4.2 ML & Analytics Layer

**Primary Role:** Transform raw tickets to embeddings, clusters, metrics, and visual insights.

**Sub-components:**

1. **Preprocessing & Normalization**

   * Script: `models/embed/compute_embeddings.py`
   * Tasks:

     * Load JSONL â†’ DataFrame
     * Normalize schema
     * Create unified `text` field (summary + description)
     * Save `tickets.parquet` and metadata

2. **Embeddings**

   * Script (same as above, second part)
   * Calls Ollamaâ€™s `/api/embeddings`:

     * Model examples: `mxbai-embed-large`, `nomic-embed-text`
   * Produces:

     * `data/processed/embeddings.npy`

3. **Manifold Projection**

   * Script: `models/cluster/cluster_umap_hdbscan.py`
   * Techniques:

     * UMAP / t-SNE / PaCMAP to project to 3D
   * Produces:

     * `data/processed/manifold_3d.npy`

4. **Clustering & Topic Extraction**

   * Script: same as above
   * Algorithm:

     * HDBSCAN (primary)
   * Produces:

     * `data/processed/cluster_labels.npy`
     * `data/processed/cluster_summary.json`

5. **Model Health & Metrics**

   * Script: `models/eval/embedding_health.py`
   * Computes:

     * silhouette score
     * cluster size distribution
     * noise fraction
     * severity distribution
     * category/product purity
   * Produces:

     * `data/processed/model_health.json`

---

## 5. Data Flow

### 5.1 Logical Data Flow

```text
[1] Synthetic Generation
    - apexgrid_tickets.jsonl

        â†“

[2] Preprocessing
    - tickets.parquet
    - ticket_meta.json

        â†“

[3] Embeddings
    - embeddings.npy

        â†“

[4] Manifold Projection
    - manifold_3d.npy

        â†“

[5] Clustering & Topics
    - cluster_labels.npy
    - cluster_summary.json

        â†“

[6] Evaluation
    - model_health.json

        â†“

[7] Export for Web
    - web/data/ticket_points.js
    - web/data/ticket_summary.js
```

### 5.2 Mermaid Overview (optional)

You can paste this into a Mermaid-compatible renderer:

```mermaid
flowchart TD
    A[Synthetic Tickets<br/>generate_tickets_ollama.py] --> B[Preprocess<br/>tickets.parquet]
    B --> C[Embeddings<br/>embeddings.npy]
    C --> D[Manifold Projection<br/>manifold_3d.npy]
    D --> E[Clustering & Topics<br/>cluster_labels.npy]
    E --> F[Model Health<br/>model_health.json]
    F --> G[Web Export<br/>ticket_points.js, ticket_summary.js]
    G --> H[WebGL & Charts<br/>index.html]
```

---

## 6. Visualization & Interaction Layer

**Primary Role:** Present the ML outputs in a visually compelling, interactive interface suitable for:

* client demos
* website sections
* POCs
* workshops

**Key Components:**

* `web/index.html`

  * Static HTML entrypoint
  * Loads:

    * Three.js
    * TailwindCSS
    * Chart.js
    * Vue.js (CDN)
    * `web/data/ticket_points.js`
    * `web/data/ticket_summary.js`

* `web/js/embedding_viz.js`

  * Creates a Three.js scene
  * Plots tickets as 3D points
  * Colors points by category, product, or severity
  * Adds interaction (hover info, rotation, zoom)

* `web/js/charts.js`

  * Uses Chart.js to render:

    * severity distributions
    * category counts
    * product mix
    * cluster metrics

* `web/js/tabs.js`

  * Simple tab or mode switching between:

    * 3D embedding
    * model health view
    * distributions
    * cluster insight panels

Deployment is as simple as serving `web/` via a static HTTP server.

---

## 7. Deployment & Runtime Environments

### 7.1 Local Development

Typical local stack:

* OS: Windows / macOS / Linux
* Python virtual environment
* Ollama running locally
* Static HTTP server for the `web` folder

Example commands:

```bash
# 1. Generate synthetic data
python synthetic/generate_tickets_ollama.py

# 2. Run embedding pipeline
python models/embed/compute_embeddings.py
python models/cluster/cluster_umap_hdbscan.py
python models/eval/embedding_health.py

# 3. Serve the web app
cd web
python -m http.server 3000
```

Then visit: `http://localhost:3000`.

### 7.2 Optional Containerization

For portability, components can be containerized:

* Container 1: Python ML stack + Ollama client
* Container 2: Ollama server with models
* Container 3: Static web server (nginx or similar)

This is not mandatory for demos but can support more advanced environments.

---

## 8. Security & Privacy Considerations

The architecture is intentionally designed so that:

* **All data is synthetic.**

  * No real customers, log lines, or metrics are used.
* **LLMs run locally via Ollama.**

  * No external calls are required.
* **No external API keys are needed** for core functionality.
* **No personal identifiers** should appear in generated text:

  * Enforced via prompting + validation.

This makes the system safe to use in:

* public talks
* recorded demos
* shared repositories
* client-facing materials (with clear â€œsynthetic dataâ€ disclaimer)

---

## 9. Extensibility

The architecture is modular. Future extensions can sit naturally alongside existing layers.

### 9.1 Additional Data Modalities

* IoT telemetry time-series for GridSense / LineaOps
* synthetic identity access logs for VaultShield
* synthetic application traces and metrics for HelioCloud
* synthetic billing or usage data for financial modeling

These can feed:

* anomaly detection models
* forecasting services
* multi-modal embeddings

### 9.2 Agent & API Layer

A minimal API (e.g., FastAPI/Flask) can sit between ML artifacts and the UI:

* New ticket â†’ embed â†’ nearest neighbors â†’ triage suggestion
* Cluster exploration â†’ fetch representative tickets â†’ LLM summary
* Incident detection â†’ highlight clusters with recent spikes

This layer would:

```text
[Frontend] â‡„ [API: triage, search, incident] â‡„ [Embeddings + Metadata]
```

### 9.3 Integration with Other Tools

The artifacts can be used in:

* notebooks (exploration, teaching)
* slide decks (screenshots of 3D embeddings & charts)
* external dashboards (e.g., Grafana, Superset)
* other web experiences

---

## 10. Summary

The Qognus Demo Platform architecture:

* starts with **synthetic ApexGrid support tickets**, generated via local LLMs
* flows through an **ML pipeline** producing embeddings, projections, clusters, and metrics
* exports into **lightweight JS payloads**
* renders in a **static, portable WebGL + chart-based UI**

It is:

* self-contained
* composable
* reproducible
* safe for public and client-facing use

This architecture forms the backbone for many future POCs, demos, and applied AI experiments within the Qognus ecosystem.

==================================================
FILE PATH: docs\data_generation_design.md
==================================================

# **Synthetic Data Generation Design â€” Qognus Demo Platform**

*ApexGrid Systems â€” Support Ticket Generation via Local LLMs (Ollama)*

---

## **1. Purpose & Scope**

This document describes how **synthetic enterprise support tickets** are generated for the fictional hybrid company **ApexGrid Systems** using **local LLMs via Ollama**.

Synthetic tickets serve as the foundation for:

* embeddings
* clustering
* topic extraction
* anomaly detection
* semantic search
* triage agent workflows
* WebGL visualization

The design ensures:

* high-quality, realistic enterprise data
* zero risk of real PII, customer names, or sensitive content
* consistent adherence to the ApexGrid taxonomy
* repeatability and controllability
* stable vocabulary for embedding clusters

---

# **2. Data Generation Goals**

Synthetic tickets must:

1. **Resemble real enterprise support cases**
2. **Use consistent domain vocabulary** from the product suite
3. **Respect the taxonomy** (category + subcategory)
4. **Produce natural cluster structure** for embeddings
5. **Include metadata** enabling charts & filters
6. **Avoid hallucinating real companies, people, or identifiable brands**
7. **Scale to 5,000â€“20,000 tickets** without quality collapse

---

# **3. Generation Pipeline Overview**

```text
[Metadata Matrix]
    â†“
[Prompt Builder]
    â†“
[Ollama Chat API]
    â†“
[JSON Validator]
    â†“
[Post-Processor / Normalizer]
    â†“
[data/raw/apexgrid_tickets.jsonl]
```

### Components:

* **Metadata Matrix**
  Pre-generated combinations of product, category, severity, etc.

* **Prompt Builder**
  Creates structured prompts for LLM ticket generation.

* **LLM (Ollama)**
  Local, deterministic-ish generation through system/user messages.

* **JSON Validator**
  Ensures well-formed JSON; re-requests if invalid.

* **Post-Processor**
  Applies cleanup, fixes inconsistencies, enforces taxonomy.

* **Output Writer**
  Appends JSON objects into `.jsonl` for downstream ML pipelines.

---

# **4. Models Used (via Ollama)**

Two local models are recommended:

### **4.1 Primary Ticket Generator**

* **Model:** `qwen2.5:latest` or `qwen2:latest`
* Why:

  * Best local LLM for structured, long-form enterprise text
  * Good JSON adherence
  * Clean domain language

### **4.2 Lightweight Alternative**

* **Model:** `llama3:8b` or `qwen:7b-instruct`
* Why:

  * Fast for generating large batches
  * Good JSON compliance with constrained prompts

---

# **5. Metadata Matrix Design**

Goal: generate diverse tickets while controlling distribution.

Structure (`metadata_matrix.csv` or generator):

| product     | category       | subcategory      | severity | customer_tier | region         | environment |
| ----------- | -------------- | ---------------- | -------- | ------------- | -------------- | ----------- |
| HelioCloud  | observability  | dashboard_error  | Sev3     | midmarket     | us-west-2      | production  |
| GridSense   | telemetry_drop | edge_offline     | Sev2     | enterprise    | eu-central-1   | production  |
| LineaOps    | integration    | PLC_driver_fault | Sev1     | enterprise    | us-east-1      | production  |
| VaultShield | authentication | MFA_failure      | Sev4     | startup       | ap-southeast-1 | staging     |
| ...         | ...            | ...              | ...      | ...           | ...            | ...         |

### Key Rules

* Respect product/domain alignment
* Severity distribution follows guidelines in `support_taxonomy.md`
* Ensure each product/category pair appears at least 100â€“300 times
* Generate 5â€“10k rows for robust manifold structure
* Metadata matrix drives ticket diversity, not randomness

---

# **6. Prompt Architecture**

Ticket generation uses **two-layer prompting**: a **system prompt** setting rules and a **user prompt** with metadata for a specific ticket.

---

## **6.1 System Prompt (Global Controls)**

```text
You are generating fully synthetic enterprise support tickets for a fictional company named ApexGrid Systems.

STRICT RULES:
- Never reference real companies, people, locations, or brands.
- Follow the ApexGrid Support Taxonomy exactly.
- Output ONLY valid JSON objects.
- Always include: ticket_id, timestamp, product, category, subcategory, severity, customer_tier, region, environment, channel, summary, description, topics.
- "description" should be 3-8 sentences.
- Use professional enterprise language.
- Do NOT include placeholder text.
- Use terminology consistent with the product domain (HelioCloud, GridSense, LineaOps, VaultShield).

The tickets represent common support issues across cloud observability, energy IoT, manufacturing robotics, and identity security analytics.
```

---

## **6.2 User Prompt (Per Ticket)**

```text
Generate ONE synthetic ApexGrid support ticket as JSON.

Use the following metadata exactly:

ticket_id: "{{ticket_id}}"
product: "{{product}}"
category: "{{category}}"
subcategory: "{{subcategory}}"
severity: "{{severity}}"
customer_tier: "{{customer_tier}}"
region: "{{region}}"
environment: "{{environment}}"
channel: "{{channel}}"

Focus the summary and description on the product domain and taxonomy definitions.
Include a list of short topic tags (2-4 items).
Timestamp should be within the last 30 days.
```

---

# **7. LLM Output Format**

Expected JSON object:

```json
{
  "ticket_id": "TCK-004381",
  "timestamp": "2025-02-12T03:21:44Z",
  "product": "HelioCloud",
  "category": "observability",
  "subcategory": "missing_log_stream",
  "severity": "Sev2",
  "customer_tier": "midmarket",
  "region": "us-west-2",
  "environment": "production",
  "channel": "portal",
  "summary": "Log stream for container group is missing from dashboard.",
  "description": "Our engineering team noticed missing logs from the main container group..."
  ,
  "topics": ["logs", "agent", "dashboard"]
}
```

---

# **8. JSON Validation and Retry Logic**

Tickets must pass validation before being accepted.

Validation checks:

1. JSON loads correctly
2. All required fields exist
3. Values comply with taxonomy
4. Summary is non-empty
5. Description ~3â€“8 sentences
6. Topics is a list
7. No real PII or unapproved proper nouns
8. No placeholder content (e.g., "lorem ipsum", "example.com")

### If any validation fails:

* Retry **up to 3 times** with a stricter corrective prompt
* Track failed items in logs

---

# **9. Post-Processing & Cleanup**

Performed after validation:

### **9.1 Normalization**

* Trim whitespace
* Canonicalize severity: `Sev1` vs `sev1`
* Convert region codes to canonical (`us-west-2`, etc.)
* Strip accidental real brand mentions (fallback filter)
* Ensure product-specific vocabulary appears

### **9.2 Content Quality Enhancements**

* Confirm summary is short (8â€“12 words)
* Confirm description uses product vocabulary
* Ensure no repetitive or duplicate text

### **9.3 Optional Augmentations**

* Automatically generate topics if missing (small term-frequency cluster)
* Add `triage_summary` using a compact LLM prompt
* Embed a sentence containing a measurable indicator (latency, voltage, throughput, etc.)

---

# **10. Ticket Volume & Batching**

Recommended volumes:

| Pipeline Purpose | Ticket Count | Notes                                   |
| ---------------- | ------------ | --------------------------------------- |
| Minimal demo     | 500â€“1,500    | Fast embeddings & UMAP                  |
| Standard demo    | 3,000â€“5,000  | Strong cluster density                  |
| Advanced         | 10,000+      | Best visual separation & topic clusters |

Batch strategy:

* Generate in chunks of 50â€“200 tickets
* Checkpoint after each batch
* Each batch receives a unique timestamp seed to avoid clustering artifacts

---

# **11. Determinism and Reproducibility**

To balance LLM creativity with repeatability:

* Set `temperature: 0.7â€“0.9` for richness
* For reproducibility experiments, use `temperature: 0.2â€“0.4`
* Persist the metadata matrix
* Persist all system and user prompts for audit
* Save each batchâ€™s generation logs under `data/logs/`

---

# **12. Data Quality Management**

### **12.1 Linguistic Quality**

* Minimum sentence length check
* No contradictions with metadata
* Product vocabulary matching:
  e.g., GridSense â†’ SCADA, sensors, voltage, telemetry

### **12.2 Taxonomy Alignment**

* Tickets must match allowed subcategories
* Cross-product mismatches are rejected:

  * e.g. VaultShield ticket containing `PLC_driver_fault` â†’ invalid
* Category/subcategory pairs validated against `support_taxonomy.md`

### **12.3 Embedding Health Considerations**

* Avoid overly repetitive phrasing
* Ensure metadata distributions match guidelines
* Insert variation via minor stochasticity

---

# **13. Scalability Considerations**

### **13.1 Multi-threading**

Generation can be parallelized:

* One thread per metadata row
* Or batched parallel calls to Ollama
* Respect Ollama model loading constraints (usually 1 model active at a time)

### **13.2 Multi-Model Strategy**

To increase diversity:

* Mix models:

  * 70% `qwen2.5:latest`
  * 30% `llama3:8b`

### **13.3 Long-term Extension**

Future datasets may include:

* synthetic IoT telemetry
* synthetic identity access logs
* synthetic SCADA packets
* synthetic error traces
* synthetic call-center transcripts

All consistent with the same fictional organization.

---

# **14. Folder Structure**

```
data_generation/
â”‚
â”œâ”€â”€ metadata_matrix.csv
â”œâ”€â”€ generate_tickets_ollama.py
â”œâ”€â”€ prompts/
â”‚   â”œâ”€â”€ system_prompt.txt
â”‚   â”œâ”€â”€ user_prompt_template.txt
â”‚   â””â”€â”€ corrective_prompt.txt
â”œâ”€â”€ validators/
â”‚   â”œâ”€â”€ json_validator.py
â”‚   â”œâ”€â”€ taxonomy_validator.py
â”‚   â””â”€â”€ pii_filter.py
â””â”€â”€ logs/
    â”œâ”€â”€ batch_0001.log
    â”œâ”€â”€ batch_0002.log
    â””â”€â”€ ...
```

---

# **15. Example Python Code Snippet**

Pseudo-implementation of a batch generator:

```python
def generate_ticket(metadata):
    payload = {
        "model": "qwen2.5:latest",
        "messages": [
            {"role": "system", "content": SYSTEM_PROMPT},
            {"role": "user", "content": build_user_prompt(metadata)}
        ]
    }

    resp = requests.post("http://localhost:11434/api/chat", json=payload)
    data = extract_json(resp.text)

    if not validate_ticket(data):
        return retry_with_correction(metadata)

    return clean_ticket(data)
```

---

# **16. Summary**

The synthetic data generation system:

* Uses **controlled metadata** to ensure domain diversity
* Uses **structured LLM prompts** to generate realistic text
* Enforces **taxonomy compliance** and vocabulary consistency
* Validates JSON and content quality
* Produces rich unified tickets across four domains
* Scales to thousands of samples
* Ensures clean semantic structure for embeddings & clustering
* Operates **100% locally** with **zero real data**

This system forms the beginning of the full ML pipeline documented in `ml_pipeline_design.md`.

==================================================
FILE PATH: docs\ml_pipeline_design.md
==================================================

# **ML Pipeline Design â€” Qognus Demo Platform**

*Embedding, clustering, evaluation, and visualization for ApexGrid synthetic data*

---

## 1. Purpose & Scope

This document describes the **end-to-end machine learning pipeline** used in the Qognus Demo Platform for the fictional hybrid enterprise **ApexGrid Systems**.

The pipeline supports:

* LLM-based **synthetic support ticket generation**
* **Text embeddings** for semantic understanding and search
* **Dimensionality reduction** and **clustering** for 2D/3D visualization
* **Model health metrics** and cluster-quality evaluation
* **Web-ready artifacts** (JavaScript payloads) for 3D WebGL and charts
* Optional **agentic workflows** (routing, triage, summarization)

The design prioritizes:

* reproducibility
* explainability
* modular components
* fully synthetic, non-sensitive data
* compatibility with local models via **Ollama**

---

## 2. High-Level Architecture

### 2.1 Pipeline Stages

The ML pipeline is organized into the following stages:

1. **Synthetic Ticket Generation** (LLM via Ollama)
2. **Preprocessing & Normalization**
3. **Embedding Computation**
4. **Manifold Projection (2D/3D)**
5. **Clustering & Topic Extraction**
6. **Model Health & Evaluation Metrics**
7. **Artifact Export (for Web UI)**
8. **Agentic & Triage Workflows** (optional extension)

Visually:

```text
[LLM Synthetic Tickets]
        â”‚
        â–¼
[Preprocess & Normalize]
        â”‚
        â–¼
[Text Embeddings]
        â”‚
        â”œâ”€â”€â”€â”€â–º [Semantic Search / Duplicate Detection]
        â”‚
        â–¼
[Dimensionality Reduction (UMAP/TSNE)]
        â”‚
        â–¼
[Clustering (HDBSCAN) & Topics]
        â”‚
        â–¼
[Model Health Metrics]
        â”‚
        â–¼
[Web Artifacts: JS payloads + charts]
        â”‚
        â–¼
[3D WebGL Viz + Dashboards + Agents]
```

---

## 3. Data Inputs & Outputs

### 3.1 Input: Synthetic Tickets

**Source:** `data/raw/apexgrid_tickets.jsonl` (or similar)

Each line contains a JSON object matching the **ApexGrid Support Ticket Schema**:

* `ticket_id`
* `timestamp`
* `product` (HelioCloud, GridSense, LineaOps, VaultShield)
* `category`, `subcategory`
* `severity` (Sev1â€“Sev4)
* `customer_tier`, `region`, `environment`
* `channel`
* `summary`, `description`
* optional `topics`

Tickets are generated via local models using **Ollama** (see `data_generation_design.md`).

### 3.2 Intermediate Outputs

* `data/processed/tickets.parquet` â€” normalized DataFrame
* `data/processed/embeddings.npy` â€” dense embedding matrix `[N x D]`
* `data/processed/ticket_meta.json` â€” metadata per ticket id
* `data/processed/manifold_3d.npy` â€” 3D projection `[N x 3]`
* `data/processed/cluster_labels.npy` â€” cluster ID per ticket
* `data/processed/model_health.json` â€” evaluation metrics

### 3.3 Web Artifacts

Under `web/data/`:

* `ticket_points.js`

  ```js
  window.TICKET_POINTS = {
    points: [
      {
        id: "TCK-000123",
        x: 0.23,
        y: -1.02,
        z: 1.84,
        product: "HelioCloud",
        category: "observability",
        severity: "Sev2",
        isP1: false,
        clusterId: 5
      },
      ...
    ]
  };
  ```

* `ticket_summary.js`

  ```js
  window.TICKET_SUMMARY = {
    numTickets: 10000,
    severityDistribution: { "Sev1": 0.03, "Sev2": 0.12, ... },
    categoryCounts: { "telemetry_drop": 1200, ... },
    clusterStats: {
      numClusters: 18,
      avgSilhouette: 0.42,
      largestClusterSize: 980,
      noiseFraction: 0.08
    }
  };
  ```

These are directly consumed by `index.html` and associated JS modules.

---

## 4. Synthetic Ticket Generation (Stage 1)

> Detailed prompting lives in `data_generation_design.md`; here we focus on pipeline integration.

**Implementation:** `synthetic/generate_tickets_ollama.py`

### 4.1 Responsibilities

* Call Ollamaâ€™s `/api/chat` with:

  * system prompt describing the schema and domain vocabulary
  * user prompt containing fixed metadata (product, severity, etc.)
* Decode and validate the JSON response
* Enforce compliance with taxonomy and schema
* Write tickets to JSONL, e.g.:

```json
{"ticket_id": "TCK-000001", "product": "HelioCloud", "category": "observability", ...}
```

### 4.2 Design Considerations

* Run in **batches** (e.g., 10â€“50 tickets at a time) for stability
* Validate `category`/`subcategory` against `support_taxonomy.md`
* Ensure no real companies or people are mentioned (prompt-level constraints)
* Keep `description` length manageable (3â€“8 sentences) to reduce embedding cost

---

## 5. Preprocessing & Normalization (Stage 2)

**Implementation:** `models/embed/compute_embeddings.py` (first half)

### 5.1 Steps

1. Read JSONL â†’ `pandas.DataFrame`

2. Enforce schema:

   * missing fields filled with defaults
   * invalid categories dropped or mapped

3. Combine `summary` + `description` into a `text` field:

   ```python
   df["text"] = df["summary"].fillna("") + " " + df["description"].fillna("")
   ```

4. Normalize metadata values:

   * severity â†’ `Sev1`, `Sev2`, â€¦
   * region â†’ canonical codes (`us-west-2`, etc.)
   * product â†’ one of four known products

5. Optionally filter out extremely short or malformed tickets.

### 5.2 Outputs

* `tickets.parquet`
* `ticket_meta.json` containing ticket id â†’ metadata mapping

---

## 6. Embedding Computation (Stage 3)

**Implementation:** `models/embed/compute_embeddings.py` (second half)
**Embedding models:** local via Ollama, e.g.:

* `mxbai-embed-large`
* `nomic-embed-text`

### 6.1 API Call Pattern

Embedding is done by hitting:

```python
def get_embedding(text: str) -> list[float]:
    resp = requests.post(
        "http://localhost:11434/api/embeddings",
        json={"model": EMBED_MODEL, "prompt": text}
    )
    resp.raise_for_status()
    return resp.json()["embedding"]
```

### 6.2 Batch Strategy

* Iterate over `df["text"]`
* For each ticket:

  * compute embedding vector
  * append to an array
* Optionally checkpoint every N tickets

### 6.3 Post-processing

* Store embeddings as `numpy.ndarray` of shape `[N, D]`
* Save to `data/processed/embeddings.npy`
* Verify no NaNs; apply standardization (optional):

  ```python
  from sklearn.preprocessing import StandardScaler
  scaler = StandardScaler()
  embeddings_scaled = scaler.fit_transform(embeddings)
  ```

---

## 7. Manifold Projection (Stage 4)

**Implementation:** `models/cluster/cluster_umap_hdbscan.py` (first part)

The goal here is to map high-dimensional embeddings into a **3D space** for WebGL, and optionally a 2D space for charts.

### 7.1 Algorithms

* Primary: **UMAP** (`umap-learn`)
* Alternative: **t-SNE** or PaCMAP for experimentation

Example (UMAP 3D):

```python
import umap
reducer = umap.UMAP(
    n_components=3,
    n_neighbors=30,
    min_dist=0.1,
    metric="cosine",
    random_state=42,
)
manifold_3d = reducer.fit_transform(embeddings_scaled)
```

### 7.2 Normalization for WebGL

After projection:

```python
max_abs = max(abs(manifold_3d).max(), 1e-6)
manifold_3d_norm = manifold_3d / max_abs * 3.0  # roughly within cube [-3, 3]^3
```

Save:

* `data/processed/manifold_3d.npy`

---

## 8. Clustering & Topic Extraction (Stage 5)

**Implementation:** `models/cluster/cluster_umap_hdbscan.py` (second part)

### 8.1 Clustering

Primary algorithm: **HDBSCAN**

```python
import hdbscan

clusterer = hdbscan.HDBSCAN(
    min_cluster_size=30,
    min_samples=10,
    metric="euclidean",
    cluster_selection_epsilon=0.05,
)
labels = clusterer.fit_predict(manifold_3d)
```

Characteristics:

* `labels[i] == -1` â†’ noise / unassigned
* Remaining labels cluster tickets into semantically related groups

Save:

* `cluster_labels.npy`

### 8.2 Cluster Summaries & Topic Hints

For each cluster:

1. Collect all tickets with that label
2. Extract top keywords using TFâ€“IDF or a simple term-frequency approach
3. Store representative words + counts

Optional: use an LLM to give each cluster a human-readable label:

```text
Cluster 5: "SCADA time synchronization drift across multiple substations."
Cluster 9: "VaultShield MFA false positives and throttling complaints."
```

Summaries stored into:

* `data/processed/cluster_summary.json`

---

## 9. Model Health & Evaluation Metrics (Stage 6)

**Implementation:** `models/eval/embedding_health.py`

The goal is to quantify the quality and behavior of the embedding + clustering.

### 9.1 Core Metrics

* **Silhouette Score**

  ```python
  from sklearn.metrics import silhouette_score
  valid_mask = labels != -1
  if valid_mask.sum() > 1:
      sil = silhouette_score(manifold_3d[valid_mask], labels[valid_mask])
  else:
      sil = None
  ```

* **Cluster Size Distribution**

  * Histogram of size per cluster
  * Fraction of points in noise

* **Category & Product Purity**

  * For each cluster, compute the distribution of:

    * `product`
    * `category`

* **Severity Distribution**

  * Weighted distribution across clusters
  * Fraction of Sev1 concentrated in particular clusters

### 9.2 Optional Retrieval Metrics

For duplicate or near-duplicate detection:

* Build approximate nearest neighbor index (e.g. FAISS or sklearn NearestNeighbors)
* Evaluate **precision@k** for retrieving tickets with identical (or similar) `category` + `subcategory`

Example:

```python
from sklearn.neighbors import NearestNeighbors

nn = NearestNeighbors(n_neighbors=6, metric="cosine").fit(embeddings_scaled)
distances, indices = nn.kneighbors(embeddings_scaled)
# For each ticket, ignore itself (index 0), inspect neighbors 1..5
```

### 9.3 Output Structure

Write results as `model_health.json`:

```json
{
  "numTickets": 10000,
  "numClusters": 18,
  "noiseFraction": 0.08,
  "avgSilhouette": 0.42,
  "largestClusterSize": 980,
  "categoryPurityByCluster": {
    "0": {"telemetry_drop": 0.74, "data_quality": 0.12, "other": 0.14},
    "1": {"observability": 0.89, "latency": 0.07, "other": 0.04}
  },
  "severityDistribution": {
    "Sev1": 0.03,
    "Sev2": 0.12,
    "Sev3": 0.40,
    "Sev4": 0.45
  }
}
```

---

## 10. Artifact Export for Web UI (Stage 7)

**Implementation:** usually part of clustering/eval scripts or a dedicated exporter, e.g. `models/eval/export_web_artifacts.py`.

### 10.1 ticket_points.js

Combine:

* `ticket_id`
* normalized 3D coordinates
* cluster label
* selected metadata fields

Example snippet:

```python
import json
import numpy as np
from pathlib import Path

def export_ticket_points(df, coords_3d, labels, out_path):
    points = []
    for i, row in df.iterrows():
        x, y, z = coords_3d[i].tolist()
        points.append({
            "id": row["ticket_id"],
            "x": float(x),
            "y": float(y),
            "z": float(z),
            "product": row["product"],
            "category": row["category"],
            "severity": row["severity"],
            "clusterId": int(labels[i]),
            "isP1": row["severity"] == "Sev1"
        })

    out = {
        "points": points
    }

    out_path.parent.mkdir(parents=True, exist_ok=True)
    with out_path.open("w", encoding="utf-8") as f:
        f.write("window.TICKET_POINTS = ")
        json.dump(out, f)
        f.write(";\n")
```

### 10.2 ticket_summary.js

Aggregate high-level metrics + distributions for charts:

```python
summary = {
  "numTickets": int(len(df)),
  "severityDistribution": severity_dist,
  "categoryCounts": category_counts,
  "productCounts": product_counts,
  "clusterStats": {
    "numClusters": num_clusters,
    "avgSilhouette": sil,
    "noiseFraction": noise_fraction
  }
}
```

Write:

```python
with open("web/data/ticket_summary.js", "w", encoding="utf-8") as f:
    f.write("window.TICKET_SUMMARY = ")
    json.dump(summary, f)
    f.write(";\n")
```

---

## 11. Agentic & Triage Workflows (Stage 8 â€“ Optional)

Beyond visualization, the same embedding + clustering setup can power **agent-style workflows**:

### 11.1 Triage Agent

* Inputs: new ticket text
* Steps:

  1. Embed the ticket
  2. Find nearest neighbors
  3. Infer likely category, severity, and product
  4. Propose:

     * suggested category/subcategory
     * similar past tickets
     * recommended resolution steps
  5. Optionally call an LLM to generate a triage summary

### 11.2 Incident Signature Detection

* Monitor cluster sizes over time
* Identify clusters whose ticket count spikes over a window (e.g., last 24/72 hours)
* Trigger a â€œpotential incidentâ€ alert for that cluster

### 11.3 Integration with Web UI

* Provide an API or local endpoint that:

  * takes free-text
  * returns:

    * 3D location
    * nearest existing points
    * recommended cluster
    * triage suggestion

This can be demoed from the browser using a local backend.

---

## 12. Tech Stack Overview

### 12.1 Backend / ML

* **Python 3.x**
* `pandas` / `numpy`
* `scikit-learn` (UMAP, TSNE via separate package, metrics)
* `umap-learn`
* `hdbscan`
* `requests` (for Ollama calls)
* Optional: `faiss` or similar for ANN

### 12.2 Data & Artifacts

* JSONL for raw data
* Parquet / NPY / JSON for processed artifacts
* JS globals for web integration (no backend needed for demo)

### 12.3 Frontend

* **Three.js** for 3D scatterplot
* **TailwindCSS** for layout and styling
* **Chart.js** for charts
* **Vue.js (CDN)** for light state management
* `index.html` served via simple static hosting or `python -m http.server`

---

## 13. Operational Considerations

### 13.1 Batch vs. Real-Time

* The current design is **batch-oriented**:

  * Generate tickets
  * Recompute embeddings
  * Re-run manifold & clustering
  * Export web artifacts

* For real-time or incremental demos, lighter-weight incremental updates could be implemented (e.g., partial embedding + local re-clustering for new tickets), but this is out of scope for the initial design.

### 13.2 Reproducibility

* Fix random seeds:

  * UMAP / TSNE random state
  * HDBSCAN randomness, if applicable
* Version file formats and schema in documentation
* Optionally log runs (e.g., using MLflow or simple JSON run logs)

---

## 14. Summary

The Qognus Demo Platformâ€™s ML pipeline for ApexGrid Systems:

* takes fully synthetic, LLM-generated support tickets
* converts them into dense embeddings suitable for

  * semantic search
  * clustering
  * visualization
* projects them into 3D for a **WebGL point cloud** that is:

  * semantically structured
  * visually compelling
  * easily explainable
* evaluates embedding and cluster quality with interpretable metrics
* exposes artifacts to a static frontend for demos, POCs, and workshops
* can be extended with triage agents, semantic search, and incident detection

This design makes ApexGrid an ideal synthetic â€œplaygroundâ€ for applied AI/ML in consulting, research, and product storytelling.

==================================================
FILE PATH: docs\product_suite.md
==================================================

# **ApexGrid Product Suite**

*Foundational fictional products for synthetic data generation & applied AI/ML demonstrations*

---

## **Overview**

ApexGrid Systems offers four major fictional platform products, each designed to mimic the diversity and complexity of real-world enterprise technology stacks. Together, they create a rich environment for LLM-generated support data, ML clustering, anomaly detection, and interactive visualization.

The product suite spans:

* SaaS observability
* energy & utility IoT telemetry
* industrial manufacturing operations
* identity & cybersecurity analytics

These domains were chosen because they produce **distinct natural language patterns**, **unique incident categories**, and **realistic ML use cases**â€”ideal for synthetic tickets, manifold embeddings, cluster separations, and topic modeling.

---

# **1. HelioCloud â€” SaaS Observability & Application Monitoring**

## **1.1 Product Summary**

**HelioCloud** is ApexGridâ€™s cloud-native observability platform, comparable to modern APM (Application Performance Monitoring) solutions. It provides monitoring and diagnostics across microservices, containers, event streams, and distributed systems.

Designed for high-growth SaaS and enterprise application teams, HelioCloud consolidates operational signals into a unified interface.

## **1.2 Core Capabilities**

* **Metrics & time-series monitoring** (CPU, memory, latency, queue depth)
* **Log ingestion & enrichment**
* **Distributed tracing** (OpenTelemetry native)
* **SLO dashboards & burn rate alerts**
* **Predictive incident signatures** using ML
* **Service dependency mapping**
* **Alerting & policy routing**

## **1.3 Common Support Issues (synthetic)**

These drive synthetic ticket generation:

* alert delays
* missing metrics
* 500 error spikes
* trace sampling misconfiguration
* log ingestion pipeline failures
* agent version mismatches
* dashboard rendering bugs
* noisy or duplicate alerts
* API rate-limit errors (429)

## **1.4 Metadata Patterns**

* Severity: Sev2â€“Sev4
* Customers: SaaS engineering teams
* Regions: globally distributed
* Channel: web portal or Slack

These create natural clusters in embeddings (observability-focused vocabulary, alert patterns, etc.).

---

# **2. GridSense â€” Energy & Utilities IoT Monitoring**

## **2.1 Product Summary**

**GridSense** monitors distributed IoT sensors across the electrical gridâ€”transformers, substations, meters, and edge energy devices. Itâ€™s designed for utility providers and energy operators requiring real-time situational awareness.

GridSense represents large physical infrastructure + IoT edge environments, generating extremely distinctive support narratives.

## **2.2 Core Capabilities**

* **Voltage & frequency anomaly detection**
* **Grid load monitoring**
* **SCADA integration (IEC 61850, DNP3)**
* **Edge gateway management**
* **Predictive maintenance**
* **Substation telemetry aggregation**
* **Power quality analytics**

## **2.3 Common Support Issues (synthetic)**

* telemetry dropouts
* SCADA desynchronization
* sensor calibration drift
* false-positive voltage anomalies
* edge gateway offline
* firmware compatibility issues
* time sync failures (PTP/NTP)
* IoT packet loss
* relay misconfiguration

## **2.4 Metadata Patterns**

* Severity: Sev1â€“Sev3
* Customers: utilities, energy operators
* Regions: geographically clustered deployments
* Channel: email or phone transcript

These produce long, technically descriptive ticketsâ€”excellent for embeddings and clustering separation.

---

# **3. LineaOps â€” Industrial Manufacturing & Robotics Cloud**

## **3.1 Product Summary**

**LineaOps** is ApexGridâ€™s industrial operations platform, focused on factory IoT ingestion, robotics telemetry, and throughput analytics. It powers continuous monitoring and predictive failure detection on manufacturing floors.

LineaOps represents real-world industrial ML domains: robotics, PLC drivers, throughput signals, automation pipelines.

## **3.2 Core Capabilities**

* **PLC driver integration (OPC-UA, Modbus)**
* **Robotics fleet monitoring**
* **Conveyor metrics & throughput analytics**
* **Predictive downtime modeling**
* **Workcell anomaly detection**
* **Firmware orchestration**
* **Operator dashboards**

## **3.3 Common Support Issues (synthetic)**

* PLC driver failures
* robot arm offline
* rising reject rates
* conveyor jitter
* throughput anomalies
* firmware update failures
* sensor misalignment
* latency spikes in workcell loops
* robotics collision safety warnings

## **3.4 Metadata Patterns**

* Severity: Sev1â€“Sev4
* Customers: manufacturing plants, robotics integrators
* Regions: local clusters per factory site
* Channel: portal or phone

These create a vocabulary-rich domain (PLC tags, robotics terminology, industrial process words) that clusters distinctly from SaaS or energy tickets.

---

# **4. VaultShield â€” Identity Security & Threat Analytics**

## **4.1 Product Summary**

**VaultShield** provides identity-centric threat detection and behavioral analytics for enterprise security teams. It focuses on authentication flows, MFA, SSO, anomaly detection, and SOC alert triage.

VaultShield gives the dataset a **cybersecurity and identity-themed ticket class**, crucial for embeddings that show clean cluster boundaries.

## **4.2 Core Capabilities**

* **Behavioral identity anomaly detection**
* **MFA & SSO monitoring**
* **Threat alert correlation**
* **Suspicious login event detection**
* **SIEM integration** (Splunk, Elastic, Azure Sentinel)
* **Zero-Trust access scoring**
* **Automated incident playbooks**

## **4.3 Common Support Issues (synthetic)**

* false-positive brute-force alerts
* MFA drift
* SSO federation failures
* suspicious IP false alarms
* SIEM connector issues
* noisy authentication logs
* OAuth token misconfiguration
* RBAC permission mismatch
* anomalous login spikes

## **4.4 Metadata Patterns**

* Severity: Sev2â€“Sev4
* Customers: SOC teams, security analysts
* Regions: global with compliance constraints
* Channel: portal or email

These tickets tend to be short, high-frequency, and repeated, excellent for demonstrating duplicate detection and semantic clustering.

---

# **5. Cross-Product Interaction Scenarios**

Because ApexGrid is hybrid, many synthetic tickets can reflect cross-product interactions, ideal for embeddings:

### **Examples**

* HelioCloud alert spikes caused by LineaOps device overload
* GridSense sensor outages causing anomalous identity logins (VaultShield)
* LineaOps firmware pushes misaligned with HelioCloud agents
* VaultShield SSO failures tied to HelioCloud API availability

These â€œmulti-product incident chainsâ€ provide:

* naturally interesting ML clusters
* cross-domain vocabulary
* realistic multi-factor root causes
* opportunities for agent-based triage workflows

---

# **6. Product-Specific Data Signatures (for ML)**

Each product creates special linguistic and structural patterns in synthetic text.

### **HelioCloud**

* microservice language
* latency metrics
* dashboards and alerts
* developer-focused vocabulary â†’ tight textual clusters

### **GridSense**

* SCADA terminology
* IoT device IDs
* edge gateways
* field conditions â†’ broader clusters, more variance

### **LineaOps**

* robotics terminology
* PLC codes
* throughput signals â†’ dense clusters with repeated patterns

### **VaultShield**

* authentication flow terms
* MFA/SSO
* IP reputation language â†’ short, frequent clusters

These properties help create visually compelling and interpretable embedding manifolds.

---

# **7. Extensibility**

Products are modular and can easily be expanded to support:

* synthetic IoT sensor streams
* synthetic SCADA logs
* synthetic access logs
* synthetic APM tracing spans
* synthetic security alerts
* synthetic cloud costs or billing events
* synthetic firmware release notes
* synthetic call-center transcripts

This ensures the product suite can evolve in any direction required by specific POCs or demonstrations.

---

# **8. Summary**

The **ApexGrid Systems Product Suite** provides four deeply differentiated domainsâ€”SaaS observability, energy IoT monitoring, manufacturing/robotics operations, and identity security.

Each product is purpose-designed to produce:

* rich synthetic support tickets
* distinct natural language clusters
* realistic operational metadata
* diverse ML behaviors (topic drift, severity trends, duplicate detection)
* meaningful 3D embedding structures

This suite is the backbone for synthetic enterprise demonstrations across the Qognus Demo Platform.

==================================================
FILE PATH: docs\support_taxonomy.md
==================================================

# **ApexGrid Support Ticket Taxonomy**

*Unified schema for synthetic ticket generation, embeddings, clustering & triage workflows*

---

## **1. Introduction**

This document defines the official **Support Ticket Taxonomy** for ApexGrid Systems.
The taxonomy is designed to:

* produce realistic synthetic support tickets
* drive clear 2D/3D semantic embedding clusters
* provide consistent metadata for evaluation
* support downstream ML tasks such as routing, topic modeling, and duplicate detection
* reflect the hybrid nature of ApexGridâ€™s product suite

All categories, subcategories, and metadata fields are fictional and safe for open-source use.

---

# **2. Ticket Data Model**

Every synthetic ticket conforms to the following schema:

| Field           | Type         | Description                                          |
| --------------- | ------------ | ---------------------------------------------------- |
| `ticket_id`     | string       | Unique ticket identifier                             |
| `timestamp`     | ISO8601      | Creation time                                        |
| `product`       | enum         | One of: HelioCloud, GridSense, LineaOps, VaultShield |
| `category`      | enum         | High-level issue area                                |
| `subcategory`   | enum         | More specific issue subtype                          |
| `severity`      | enum         | Sev1â€“Sev4                                            |
| `customer_tier` | enum         | enterprise, midmarket, startup                       |
| `region`        | enum         | us-west-2, us-east-1, eu-central-1, ap-southeast-1   |
| `environment`   | enum         | production, staging, sandbox                         |
| `channel`       | enum         | email, portal, slack, phone                          |
| `summary`       | string       | Short description                                    |
| `description`   | string       | Full natural-language body                           |
| `topics`        | list<string> | Optional LLM-generated topic hints                   |

This structure ensures strong ML signal while remaining flexible.

---

# **3. Category Overview (Top-Level)**

ApexGrid uses 14 top-level categories spanning SaaS, energy, industrial, and security domains.

These drive major cluster separations in embeddings.

---

## **3.1 authentication**

Issues related to user login, MFA, or identity lifecycle.

**Examples**

* MFA_failure
* SSO_drift
* password_reset_loop
* oauth_token_expired
* unexpected_logout

**Products:** VaultShield, HelioCloud

---

## **3.2 authorization**

RBAC, permissions, and access policy errors.

**Examples**

* permission_denied
* role_mismatch
* policy_conflict
* API_scope_invalid

**Products:** VaultShield, HelioCloud

---

## **3.3 billing**

Financial and usage-related issues.

**Examples**

* invoice_discrepancy
* overage_dispute
* credit_allocation
* subscription_tier_mismatch

**Products:** HelioCloud

---

## **3.4 latency**

Performance or response time degradation.

**Examples**

* p95_spike
* slow_dashboard_render
* trace_latency_regression
* edge_roundtrip_high

**Products:** HelioCloud, LineaOps, GridSense

---

## **3.5 integration**

Failures in third-party or internal system connectors.

**Examples**

* SIEM_connector_failed
* SCADA_protocol_error
* PLC_driver_fault
* SSO_integration_error
* webhook_delivery_failed

**Products:** All, but domain varies

* LineaOps â†’ PLC/OPC-UA
* GridSense â†’ SCADA
* VaultShield â†’ SIEM/SSO
* HelioCloud â†’ webhook/API

---

## **3.6 data_quality**

Issues related to data correctness, completeness, or drift.

**Examples**

* missing_values
* schema_mismatch
* drift_detected
* metric_cardinality_explosion
* timestamp_skew

**Products:** All

---

## **3.7 api_errors**

Errors surfaced via API calls.

**Examples**

* rate_limit_429
* internal_500
* auth_failed_401
* payload_too_large
* malformed_request

**Products:** HelioCloud, VaultShield

---

## **3.8 telemetry_drop**

Loss of sensor, agent, or device data.

**Examples**

* sensor_unreachable
* edge_offline
* gateway_loss
* backlog_spike
* ingestion_gap

**Products:** GridSense, LineaOps, HelioCloud (agent data)

---

## **3.9 security_alerts**

Threat or identity events that triggered analytical modules.

**Examples**

* bruteforce_detected
* anomalous_login
* malicious_IP
* impossible_travel
* MFA_bypass_suspected

**Products:** VaultShield

---

## **3.10 observability**

Dashboards, logs, traces, metrics, and alerting.

**Examples**

* missing_log_stream
* dashboard_error
* alert_deduping_failed
* trace_sampling_bug

**Products:** HelioCloud

---

## **3.11 deployment_failures**

Issues with pushing configuration, firmware, or software updates.

**Examples**

* canary_failed
* rollout_aborted
* edge_firmware_timeout
* agent_upgrade_failed
* PLC_update_incomplete

**Products:** LineaOps, GridSense, HelioCloud

---

## **3.12 firmware**

Hardware-level compatibility issues.

**Examples**

* version_mismatch
* checksum_failure
* hardware_cap_exceeded
* unsupported_firmware

**Products:** LineaOps, GridSense

---

## **3.13 scaling**

Problems with workload capacity or autoscaling mechanisms.

**Examples**

* autoscaler_unresponsive
* pod_eviction_spike
* storage_saturation
* scaling_policy_conflict

**Products:** HelioCloud

---

## **3.14 dashboard_issues**

User-interface or visualisation-related errors.

**Examples**

* widget_failure
* stale_chart
* incorrect_units
* access_denied_in_ui

**Products:** HelioCloud, VaultShield

---

# **4. Subcategory Dictionary**

Below is the consolidated mapping:

```
authentication:
  - MFA_failure
  - SSO_drift
  - oauth_token_expired
  - unexpected_logout
  - credential_validation_error

authorization:
  - permission_denied
  - role_mismatch
  - policy_conflict
  - invalid_scope

billing:
  - invoice_discrepancy
  - duplicate_charge
  - credit_allocation
  - subscription_tier_mismatch

latency:
  - p95_spike
  - trace_delay
  - dashboard_render_slow
  - edge_roundtrip_high

integration:
  - SIEM_connector_failed
  - SCADA_protocol_error
  - PLC_driver_fault
  - webhook_delivery_failed
  - SSO_integration_error

data_quality:
  - missing_values
  - schema_mismatch
  - drift_detected
  - timestamp_skew
  - cardinality_explosion

api_errors:
  - 429_rate_limit
  - 500_internal
  - 401_unauthorized
  - payload_too_large
  - malformed_request

telemetry_drop:
  - sensor_unreachable
  - edge_offline
  - gateway_loss
  - ingestion_gap
  - backlog_spike

security_alerts:
  - bruteforce_detected
  - anomalous_login
  - malicious_IP
  - impossible_travel
  - MFA_bypass_suspected

observability:
  - missing_log_stream
  - dashboard_error
  - trace_sampling_bug
  - alert_deduping_failed

deployment_failures:
  - canary_failed
  - rollout_aborted
  - agent_upgrade_failed
  - PLC_update_incomplete

firmware:
  - version_mismatch
  - checksum_failure
  - unsupported_firmware
  - incompatible_module

scaling:
  - autoscaler_unresponsive
  - storage_saturation
  - pod_evicted
  - scaling_policy_mismatch

dashboard_issues:
  - widget_failure
  - stale_chart
  - incorrect_units
  - rendering_error
```

This dictionary can be referenced directly by LLM prompts, embedding pipelines, or dashboards.

---

# **5. Metadata Distribution Guidelines**

This section defines how synthetic data should be balanced for realistic domain simulation.

## **Severity Distribution**

* **Sev1:** 3%
* **Sev2:** 12%
* **Sev3:** 40%
* **Sev4:** 45%

## **Product Distribution**

Ideal for clustering variety:

* HelioCloud â€” 40%
* GridSense â€” 25%
* LineaOps â€” 20%
* VaultShield â€” 15%

## **Customer Tier**

* enterprise â€” 40%
* midmarket â€” 40%
* startup â€” 20%

## **Channels**

* portal â€” 45%
* email â€” 30%
* slack â€” 15%
* phone_transcript â€” 10%

## **Regions**

* us-west-2 â€” 30%
* us-east-1 â€” 25%
* eu-central-1 â€” 25%
* ap-southeast-1 â€” 20%

These ratios ensure natural cluster density when projecting embeddings.

---

# **6. Triage Workflow Attributes (Optional Extensions)**

Tickets may include optional fields for richer ML demos:

* `predicted_category` (LLM-routing)
* `similar_ticket_ids`
* `requires_escalation`
* `sla_violation_likelihood`
* `related_incident_signature`
* `triage_summary` (LLM-generated)

These fields enable agentic workflows and multi-step automation prototypes.

---

# **7. Topic Keywords for LLM Consistency**

Each product area has domain-specific vocabulary. These help keep synthetic tickets consistent, realistic, and clusterable.

### **HelioCloud (SaaS Observability)**

`p95`, `latency`, `logs`, `traces`, `OpenTelemetry`, `deploy`, `dashboard`, `SLO`, `alert`, `microservice`, `pipeline`, `throughput`, `cardinality`

### **GridSense (Energy IoT)**

`voltage`, `frequency`, `SCADA`, `IEC`, `transformer`, `PTP`, `edge`, `gateway`, `sensor`, `telemetry`, `outage`, `firmware`, `reactive load`

### **LineaOps (Manufacturing)**

`PLC`, `Modbus`, `robot arm`, `workcell`, `reject rate`, `jitter`, `conveyor`, `downtime`, `cycle time`, `encoder`, `firmware push`

### **VaultShield (Identity Security)**

`MFA`, `SSO`, `OAuth`, `SIEM`, `bruteforce`, `anomalous login`, `federation`, `identity risk`, `threat score`, `audit log`

These keywords help drive uniform cluster structure across synthetic datasets.

---

# **8. Summary**

The ApexGrid Support Taxonomy provides a unified cross-product classification system for generating synthetic enterprise support tickets. It creates:

* strong signal for embeddings
* clear subcluster separation
* consistent LLM output
* realistic operational scenarios
* flexible metadata for modelling

This taxonomy is the backbone of the Qognus Demo Platformâ€™s NLP and embeddings pipeline, ensuring repeatable and believable synthetic data across all product domains.


==================================================
FILE PATH: models\cluster\cluster_umap_hdbscan.py
==================================================

"""
cluster_umap_hdbscan.py
Qognus Demo Platform â€” ApexGrid Systems
---------------------------------------

Takes precomputed embeddings for synthetic ApexGrid support tickets and:

1. Loads ticket metadata + embeddings
2. Computes a 3D manifold projection using UMAP
3. Runs HDBSCAN clustering on the 3D manifold
4. Generates lightweight cluster summaries
5. Saves:
   - manifold_3d.npy
   - cluster_labels.npy
   - cluster_summary.json

This script is designed to match the pipeline defined in:
- docs/ml_pipeline_design.md
- docs/architecture.md
"""

import json
import pathlib
from collections import Counter

import numpy as np
import pandas as pd

import umap
import hdbscan
from sklearn.feature_extraction.text import TfidfVectorizer


# ------------------------------------------------------------
# CONFIGURATION
# ------------------------------------------------------------

DATA_DIR = pathlib.Path("data")
PROCESSED_DIR = DATA_DIR / "processed"

TICKETS_PARQUET = PROCESSED_DIR / "tickets.parquet"
EMBED_NPY = PROCESSED_DIR / "embeddings.npy"

OUT_MANIFOLD = PROCESSED_DIR / "manifold_3d.npy"
OUT_LABELS = PROCESSED_DIR / "cluster_labels.npy"
OUT_CLUSTER_SUMMARY = PROCESSED_DIR / "cluster_summary.json"

# UMAP parameters â€” tweak for aesthetics vs. structure
UMAP_N_COMPONENTS = 3
UMAP_N_NEIGHBORS = 30
UMAP_MIN_DIST = 0.1
UMAP_METRIC = "cosine"
UMAP_RANDOM_STATE = 42

# HDBSCAN parameters â€” tweak for cluster granularity
HDBSCAN_MIN_CLUSTER_SIZE = 30
HDBSCAN_MIN_SAMPLES = 10
HDBSCAN_METRIC = "euclidean"
HDBSCAN_SELECTION_EPS = 0.05


# ------------------------------------------------------------
# HELPERS
# ------------------------------------------------------------

def load_data():
    """Load tickets DataFrame and embeddings matrix."""
    if not TICKETS_PARQUET.exists():
        raise FileNotFoundError(
            f"Missing {TICKETS_PARQUET}. Run compute_embeddings.py first."
        )
    if not EMBED_NPY.exists():
        raise FileNotFoundError(
            f"Missing {EMBED_NPY}. Run compute_embeddings.py first."
        )

    print(f"Loading tickets from: {TICKETS_PARQUET}")
    df = pd.read_parquet(TICKETS_PARQUET)

    print(f"Loading embeddings from: {EMBED_NPY}")
    embeddings = np.load(EMBED_NPY)

    if len(df) != embeddings.shape[0]:
        raise ValueError(
            f"Row count mismatch: tickets={len(df)} vs embeddings={embeddings.shape[0]}"
        )

    return df, embeddings


def compute_umap_3d(embeddings: np.ndarray) -> np.ndarray:
    """Compute a 3D UMAP projection."""
    print("Computing UMAP 3D projection...")
    reducer = umap.UMAP(
        n_components=UMAP_N_COMPONENTS,
        n_neighbors=UMAP_N_NEIGHBORS,
        min_dist=UMAP_MIN_DIST,
        metric=UMAP_METRIC,
        random_state=UMAP_RANDOM_STATE,
    )
    coords_3d = reducer.fit_transform(embeddings)
    print("UMAP completed.")

    # Normalize to a reasonable cube for WebGL (approx [-3, 3]^3)
    max_abs = max(abs(coords_3d).max(), 1e-6)
    coords_norm = coords_3d / max_abs * 3.0

    return coords_norm


def run_hdbscan(coords_3d: np.ndarray) -> np.ndarray:
    """Run HDBSCAN clustering on 3D coordinates."""
    print("Running HDBSCAN clustering...")
    clusterer = hdbscan.HDBSCAN(
        min_cluster_size=HDBSCAN_MIN_CLUSTER_SIZE,
        min_samples=HDBSCAN_MIN_SAMPLES,
        metric=HDBSCAN_METRIC,
        cluster_selection_epsilon=HDBSCAN_SELECTION_EPS,
    )
    labels = clusterer.fit_predict(coords_3d)

    num_clusters = len(set(labels)) - (1 if -1 in labels else 0)
    noise_fraction = float((labels == -1).sum()) / len(labels)

    print(f"HDBSCAN found {num_clusters} clusters.")
    print(f"Noise fraction: {noise_fraction:.3f}")

    return labels


def summarize_clusters(df: pd.DataFrame, labels: np.ndarray, max_terms: int = 8):
    """
    Build a simple summary for each cluster:
    - size
    - dominant product
    - dominant category
    - top keywords from TF-IDF
    """
    print("Building cluster summaries...")

    # Attach labels to DataFrame
    df = df.copy()
    df["clusterId"] = labels

    # Prepare text for TF-IDF (summary + description already in df["text"])
    texts = df["text"].fillna("").tolist()
    vectorizer = TfidfVectorizer(
        max_features=5000,
        ngram_range=(1, 2),
        stop_words="english"
    )
    tfidf = vectorizer.fit_transform(texts)
    vocab = np.array(vectorizer.get_feature_names_out())

    cluster_ids = sorted(set(labels))
    summaries = {}

    for cid in cluster_ids:
        if cid == -1:
            # treat -1 as noise; optionally summarise or skip
            continue

        mask = df["clusterId"] == cid
        idx = np.where(mask.values)[0]
        if len(idx) == 0:
            continue

        cluster_df = df.loc[mask]

        # Basic stats
        size = int(mask.sum())
        product_counts = Counter(cluster_df["product"])
        category_counts = Counter(cluster_df["category"])

        top_product, top_product_count = product_counts.most_common(1)[0]
        top_category, top_category_count = category_counts.most_common(1)[0]

        # Top terms via column-summed TF-IDF
        sub_tfidf = tfidf[idx]
        # sum across documents
        col_sum = np.asarray(sub_tfidf.sum(axis=0)).ravel()
        top_term_idx = col_sum.argsort()[::-1][:max_terms]
        top_terms = vocab[top_term_idx].tolist()

        summaries[str(cid)] = {
            "size": size,
            "topProduct": top_product,
            "topProductFraction": top_product_count / size,
            "topCategory": top_category,
            "topCategoryFraction": top_category_count / size,
            "topTerms": top_terms,
        }

    # Noise summary (optional)
    noise_mask = df["clusterId"] == -1
    noise_size = int(noise_mask.sum())
    if noise_size > 0:
        summaries["-1"] = {
            "size": noise_size,
            "topProduct": None,
            "topProductFraction": None,
            "topCategory": None,
            "topCategoryFraction": None,
            "topTerms": [],
        }

    return summaries


# ------------------------------------------------------------
# MAIN
# ------------------------------------------------------------

def main():
    print("==============================================")
    print(" Qognus Demo Platform â€” UMAP + HDBSCAN")
    print("==============================================")

    df, embeddings = load_data()

    # UMAP 3D projection
    coords_3d = compute_umap_3d(embeddings)
    OUT_MANIFOLD.parent.mkdir(parents=True, exist_ok=True)
    np.save(OUT_MANIFOLD, coords_3d)
    print(f"Saved 3D manifold â†’ {OUT_MANIFOLD}")

    # HDBSCAN clustering
    labels = run_hdbscan(coords_3d)
    np.save(OUT_LABELS, labels.astype(np.int32))
    print(f"Saved cluster labels â†’ {OUT_LABELS}")

    # Cluster summaries
    cluster_summary = summarize_clusters(df, labels)
    with OUT_CLUSTER_SUMMARY.open("w", encoding="utf-8") as f:
        json.dump(cluster_summary, f, indent=2)
    print(f"Saved cluster summary â†’ {OUT_CLUSTER_SUMMARY}")

    print("\nDone.\n")


if __name__ == "__main__":
    main()


==================================================
FILE PATH: models\embed\compute_embeddings.py
==================================================

"""
compute_embeddings.py
Qognus Demo Platform â€” ApexGrid Systems
---------------------------------------

Loads synthetic ApexGrid support tickets, normalizes fields, computes text
embeddings via a local Ollama model, and stores:
- tickets.parquet
- ticket_meta.json
- embeddings.npy
- embedding_ids.json

This script is designed to match the pipeline defined in:
docs/ml_pipeline_design.md
docs/support_taxonomy.md
docs/data_generation_design.md
"""

import json
import time
import pathlib
import requests
import numpy as np
import pandas as pd
from tqdm import tqdm

# ------------------------------------------------------------
# CONFIGURATION
# ------------------------------------------------------------

DATA_DIR = pathlib.Path("data")
RAW_TICKETS = DATA_DIR / "raw" / "apexgrid_tickets.jsonl"

PROCESSED_DIR = DATA_DIR / "processed"
PROCESSED_DIR.mkdir(parents=True, exist_ok=True)

OUT_PARQUET = PROCESSED_DIR / "tickets.parquet"
OUT_META = PROCESSED_DIR / "ticket_meta.json"
OUT_EMBED = PROCESSED_DIR / "embeddings.npy"
OUT_IDS = PROCESSED_DIR / "embedding_ids.json"

OLLAMA_URL = "http://localhost:11434/api/embeddings"
EMBED_MODEL = "mxbai-embed-large"    # or "nomic-embed-text"


# ------------------------------------------------------------
# UTILS
# ------------------------------------------------------------

def load_jsonl(path: pathlib.Path) -> list[dict]:
    """Load JSON lines file."""
    items = []
    with path.open("r", encoding="utf-8") as f:
        for line in f:
            line = line.strip()
            if not line:
                continue
            try:
                obj = json.loads(line)
                items.append(obj)
            except json.JSONDecodeError:
                print(f"Skipping malformed JSON line: {line[:120]}...")
    return items


def normalize_ticket(t: dict) -> dict:
    """Normalize one ticketâ€™s fields based on defined data model."""
    # Ensure required fields exist
    required = [
        "ticket_id", "timestamp", "product", "category", "subcategory",
        "severity", "customer_tier", "region", "environment", "channel",
        "summary", "description"
    ]
    for field in required:
        if field not in t:
            t[field] = ""

    # Canonical severity
    sev = t["severity"].strip()
    if not sev.startswith("Sev"):
        # try capitalizing existing numeric field
        digits = "".join(c for c in sev if c.isdigit())
        sev = f"Sev{digits}" if digits else "Sev4"
    t["severity"] = sev

    # Canonical region (fallback)
    t["region"] = t["region"].strip().lower()

    # Combine text fields
    summary = t["summary"] or ""
    description = t["description"] or ""
    t["text"] = (summary + " " + description).strip()

    return t


def get_embedding_ollama(text: str, model: str = EMBED_MODEL) -> list[float]:
    """Call Ollama embedding endpoint. Returns vector."""
    try:
        resp = requests.post(
            OLLAMA_URL,
            json={"model": model, "prompt": text},
            timeout=30
        )
        resp.raise_for_status()
        data = resp.json()
        return data["embedding"]
    except Exception as e:
        print(f"[ERROR] Embedding failed: {e}. Retrying in 2 seconds.")
        time.sleep(2)
        # single retry
        resp = requests.post(
            OLLAMA_URL,
            json={"model": model, "prompt": text},
            timeout=30
        )
        resp.raise_for_status()
        return resp.json()["embedding"]


# ------------------------------------------------------------
# MAIN PIPELINE
# ------------------------------------------------------------

def main():
    print("==============================================")
    print(" Qognus Demo Platform â€” Compute Embeddings")
    print("==============================================")
    print(f"Loading tickets from: {RAW_TICKETS}")

    if not RAW_TICKETS.exists():
        raise FileNotFoundError(
            f"Cannot find synthetic ticket file at {RAW_TICKETS}. "
            f"Ensure you ran generate_tickets_ollama.py first."
        )

    raw_items = load_jsonl(RAW_TICKETS)
    if not raw_items:
        raise RuntimeError("No valid tickets found. Aborting.")

    print(f"Loaded {len(raw_items)} tickets.")

    # Normalize
    print("Normalizing...")
    normed = [normalize_ticket(t) for t in raw_items]
    df = pd.DataFrame(normed)

    # Save metadata (ticket_id -> metadata)
    print("Saving metadata...")
    meta_dict = {}
    for _, row in df.iterrows():
        meta_dict[row["ticket_id"]] = {
            "product": row["product"],
            "category": row["category"],
            "subcategory": row["subcategory"],
            "severity": row["severity"],
            "customer_tier": row["customer_tier"],
            "region": row["region"],
            "environment": row["environment"],
            "channel": row["channel"],
            "timestamp": row["timestamp"]
        }

    with OUT_META.open("w", encoding="utf-8") as f:
        json.dump(meta_dict, f, indent=2)

    # Save parquet
    df.to_parquet(OUT_PARQUET, index=False)
    print(f"Saved tickets.parquet â†’ {OUT_PARQUET}")

    # ---------------------------------------------------------
    # Compute embeddings
    # ---------------------------------------------------------
    print("\nComputing embeddings from Ollama â€¦")
    texts = df["text"].tolist()
    ticket_ids = df["ticket_id"].tolist()

    all_embeddings = []
    for text in tqdm(texts, desc="Embedding"):
        vec = get_embedding_ollama(text)
        all_embeddings.append(vec)

    emb_matrix = np.array(all_embeddings, dtype=np.float32)

    # Save embeddings & id alignment
    np.save(OUT_EMBED, emb_matrix)
    with OUT_IDS.open("w", encoding="utf-8") as f:
        json.dump(ticket_ids, f, indent=2)

    print(f"\nSaved embeddings â†’ {OUT_EMBED}")
    print(f"Saved embedding_ids â†’ {OUT_IDS}")

    print("\nDone.\n")


# ------------------------------------------------------------
# ENTRYPOINT
# ------------------------------------------------------------

if __name__ == "__main__":
    main()


==================================================
FILE PATH: models\eval\embedding_health.py
==================================================

"""
embedding_health.py
Qognus Demo Platform â€” ApexGrid Systems
---------------------------------------

Evaluates the quality and behavior of the embedding + clustering pipeline.

Loads:
- data/processed/tickets.parquet
- data/processed/embeddings.npy
- data/processed/manifold_3d.npy
- data/processed/cluster_labels.npy

Computes:
- overall silhouette score (on non-noise points)
- noise fraction
- cluster size distribution
- per-cluster product & category purity
- global severity distribution

Writes:
- data/processed/model_health.json

This script matches the design described in:
- docs/ml_pipeline_design.md
- docs/architecture.md
"""

import json
import pathlib
from collections import Counter, defaultdict

import numpy as np
import pandas as pd
from sklearn.metrics import silhouette_score

# ------------------------------------------------------------
# PATHS
# ------------------------------------------------------------

DATA_DIR = pathlib.Path("data")
PROCESSED_DIR = DATA_DIR / "processed"

TICKETS_PARQUET = PROCESSED_DIR / "tickets.parquet"
EMBED_NPY = PROCESSED_DIR / "embeddings.npy"
MANIFOLD_3D_NPY = PROCESSED_DIR / "manifold_3d.npy"
CLUSTER_LABELS_NPY = PROCESSED_DIR / "cluster_labels.npy"

OUT_MODEL_HEALTH = PROCESSED_DIR / "model_health.json"


# ------------------------------------------------------------
# HELPERS
# ------------------------------------------------------------

def load_artifacts():
    """Load tickets, embeddings, 3D manifold, and cluster labels."""
    if not TICKETS_PARQUET.exists():
        raise FileNotFoundError(
            f"Missing {TICKETS_PARQUET}. Run compute_embeddings.py first."
        )
    if not EMBED_NPY.exists():
        raise FileNotFoundError(
            f"Missing {EMBED_NPY}. Run compute_embeddings.py first."
        )
    if not MANIFOLD_3D_NPY.exists():
        raise FileNotFoundError(
            f"Missing {MANIFOLD_3D_NPY}. Run cluster_umap_hdbscan.py first."
        )
    if not CLUSTER_LABELS_NPY.exists():
        raise FileNotFoundError(
            f"Missing {CLUSTER_LABELS_NPY}. Run cluster_umap_hdbscan.py first."
        )

    print(f"Loading tickets from: {TICKETS_PARQUET}")
    df = pd.read_parquet(TICKETS_PARQUET)

    print(f"Loading embeddings from: {EMBED_NPY}")
    embeddings = np.load(EMBED_NPY)

    print(f"Loading manifold 3D from: {MANIFOLD_3D_NPY}")
    coords_3d = np.load(MANIFOLD_3D_NPY)

    print(f"Loading cluster labels from: {CLUSTER_LABELS_NPY}")
    labels = np.load(CLUSTER_LABELS_NPY)

    if len(df) != embeddings.shape[0]:
        raise ValueError(
            f"Row count mismatch: tickets={len(df)} vs embeddings={embeddings.shape[0]}"
        )
    if len(df) != coords_3d.shape[0]:
        raise ValueError(
            f"Row count mismatch: tickets={len(df)} vs coords={coords_3d.shape[0]}"
        )
    if len(df) != labels.shape[0]:
        raise ValueError(
            f"Row count mismatch: tickets={len(df)} vs labels={labels.shape[0]}"
        )

    return df, embeddings, coords_3d, labels


def compute_silhouette(coords_3d: np.ndarray, labels: np.ndarray) -> float | None:
    """
    Compute silhouette score on non-noise points.
    Returns None if not enough clusters.
    """
    unique_labels = set(labels) - {-1}
    if len(unique_labels) < 2:
        print("Not enough non-noise clusters for silhouette score.")
        return None

    valid_mask = labels != -1
    if valid_mask.sum() < 2:
        print("Too few valid points for silhouette score.")
        return None

    try:
        score = silhouette_score(coords_3d[valid_mask], labels[valid_mask])
        return float(score)
    except Exception as e:
        print(f"Silhouette score computation failed: {e}")
        return None


def compute_cluster_stats(df: pd.DataFrame, labels: np.ndarray):
    """
    Compute cluster-level stats:
    - size
    - product purity
    - category purity
    """
    df = df.copy()
    df["clusterId"] = labels

    cluster_ids = sorted(set(labels))
    cluster_sizes = {}
    product_purity = {}
    category_purity = {}

    for cid in cluster_ids:
        mask = df["clusterId"] == cid
        size = int(mask.sum())
        cluster_sizes[str(cid)] = size

        if size == 0:
            product_purity[str(cid)] = {}
            category_purity[str(cid)] = {}
            continue

        sub = df.loc[mask]
        prod_counts = Counter(sub["product"].fillna("unknown"))
        cat_counts = Counter(sub["category"].fillna("unknown"))

        # Convert to fractions
        product_purity[str(cid)] = {
            prod: count / size for prod, count in prod_counts.items()
        }
        category_purity[str(cid)] = {
            cat: count / size for cat, count in cat_counts.items()
        }

    return cluster_sizes, product_purity, category_purity


def compute_severity_distribution(df: pd.DataFrame):
    """Compute global severity distribution across all tickets."""
    sev_counts = Counter(df["severity"].fillna("unknown"))
    n = len(df)
    return {sev: count / n for sev, count in sev_counts.items()}


def compute_label_counts(labels: np.ndarray):
    """Compute counts and noise fraction for cluster labels."""
    n = len(labels)
    noise_mask = labels == -1
    noise_fraction = float(noise_mask.sum()) / float(n)

    cluster_counts = Counter(l for l in labels if l != -1)
    num_clusters = len(cluster_counts)
    largest_cluster_size = max(cluster_counts.values()) if cluster_counts else 0

    return {
        "numClusters": int(num_clusters),
        "noiseFraction": noise_fraction,
        "largestClusterSize": int(largest_cluster_size),
        "clusterCounts": {str(cid): int(sz) for cid, sz in cluster_counts.items()},
    }


# ------------------------------------------------------------
# MAIN
# ------------------------------------------------------------

def main():
    print("==============================================")
    print(" Qognus Demo Platform â€” Embedding Health")
    print("==============================================")

    df, embeddings, coords_3d, labels = load_artifacts()
    n_tickets = len(df)

    print("Computing silhouette score...")
    avg_silhouette = compute_silhouette(coords_3d, labels)
    if avg_silhouette is not None:
        print(f"Average silhouette: {avg_silhouette:.3f}")
    else:
        print("Average silhouette: None")

    print("Computing cluster stats...")
    cluster_sizes, product_purity, category_purity = compute_cluster_stats(df, labels)

    print("Computing label distribution...")
    label_stats = compute_label_counts(labels)

    print("Computing severity distribution...")
    severity_distribution = compute_severity_distribution(df)

    # Assemble health dict
    health = {
        "numTickets": int(n_tickets),
        "avgSilhouette": avg_silhouette,
        "noiseFraction": label_stats["noiseFraction"],
        "numClusters": label_stats["numClusters"],
        "largestClusterSize": label_stats["largestClusterSize"],
        "clusterSizes": cluster_sizes,
        "severityDistribution": severity_distribution,
        "productPurityByCluster": product_purity,
        "categoryPurityByCluster": category_purity,
    }

    OUT_MODEL_HEALTH.parent.mkdir(parents=True, exist_ok=True)
    with OUT_MODEL_HEALTH.open("w", encoding="utf-8") as f:
        json.dump(health, f, indent=2)

    print(f"\nSaved model health â†’ {OUT_MODEL_HEALTH}")
    print("Done.\n")


if __name__ == "__main__":
    main()


==================================================
FILE PATH: models\eval\export_web_artifacts.py
==================================================

"""
export_web_artifacts.py
Qognus Demo Platform â€” ApexGrid Systems
---------------------------------------

Loads processed ApexGrid ML artifacts and exports them as
web-ready JavaScript payloads:

- web/data/ticket_points.js
    window.TICKET_POINTS = { points: [...] }

- web/data/ticket_summary.js
    window.TICKET_SUMMARY = { ... }

These are consumed directly by the static web front-end
(index.html, embedding_viz.js, charts.js, etc.).
"""

import json
import pathlib
from collections import Counter

import numpy as np
import pandas as pd
from sklearn.metrics import silhouette_score

# ------------------------------------------------------------
# PATHS
# ------------------------------------------------------------

DATA_DIR = pathlib.Path("data")
PROCESSED_DIR = DATA_DIR / "processed"

TICKETS_PARQUET = PROCESSED_DIR / "tickets.parquet"
MANIFOLD_3D_NPY = PROCESSED_DIR / "manifold_3d.npy"
CLUSTER_LABELS_NPY = PROCESSED_DIR / "cluster_labels.npy"
MODEL_HEALTH_JSON = PROCESSED_DIR / "model_health.json"  # optional

WEB_DIR = pathlib.Path("web")
WEB_DATA_DIR = WEB_DIR / "data"
WEB_DATA_DIR.mkdir(parents=True, exist_ok=True)

OUT_POINTS_JS = WEB_DATA_DIR / "ticket_points.js"
OUT_SUMMARY_JS = WEB_DATA_DIR / "ticket_summary.js"


# ------------------------------------------------------------
# HELPERS
# ------------------------------------------------------------

def load_core_artifacts():
    """Load tickets DataFrame, 3D manifold coordinates, and cluster labels."""
    if not TICKETS_PARQUET.exists():
        raise FileNotFoundError(
            f"Missing {TICKETS_PARQUET}. Run compute_embeddings.py first."
        )
    if not MANIFOLD_3D_NPY.exists():
        raise FileNotFoundError(
            f"Missing {MANIFOLD_3D_NPY}. Run cluster_umap_hdbscan.py first."
        )
    if not CLUSTER_LABELS_NPY.exists():
        raise FileNotFoundError(
            f"Missing {CLUSTER_LABELS_NPY}. Run cluster_umap_hdbscan.py first."
        )

    print(f"Loading tickets from: {TICKETS_PARQUET}")
    df = pd.read_parquet(TICKETS_PARQUET)

    print(f"Loading manifold 3D from: {MANIFOLD_3D_NPY}")
    coords_3d = np.load(MANIFOLD_3D_NPY)

    print(f"Loading cluster labels from: {CLUSTER_LABELS_NPY}")
    labels = np.load(CLUSTER_LABELS_NPY)

    if len(df) != coords_3d.shape[0]:
        raise ValueError(
            f"Row count mismatch: tickets={len(df)} vs coords={coords_3d.shape[0]}"
        )
    if len(df) != labels.shape[0]:
        raise ValueError(
            f"Row count mismatch: tickets={len(df)} vs labels={labels.shape[0]}"
        )

    return df, coords_3d, labels


def maybe_load_model_health(manifold_3d, labels):
    """
    Try to load precomputed model_health.json.
    If not found, compute a minimal set of metrics inline.
    """
    if MODEL_HEALTH_JSON.exists():
        print(f"Loading model health from: {MODEL_HEALTH_JSON}")
        with MODEL_HEALTH_JSON.open("r", encoding="utf-8") as f:
            return json.load(f)

    print("model_health.json not found; computing basic metrics inline...")

    n = len(labels)
    noise_mask = labels == -1
    noise_fraction = float(noise_mask.sum()) / float(n)

    # Compute silhouette only on non-noise labels, if possible
    unique_labels = set(labels) - {-1}
    if len(unique_labels) >= 2:
        valid_mask = labels != -1
        try:
            avg_silhouette = float(
                silhouette_score(manifold_3d[valid_mask], labels[valid_mask])
            )
        except Exception as e:
            print(f"Silhouette computation failed: {e}")
            avg_silhouette = None
    else:
        avg_silhouette = None

    # Cluster sizes (excluding noise)
    cluster_sizes = Counter(l for l in labels if l != -1)
    largest_cluster_size = max(cluster_sizes.values()) if cluster_sizes else 0
    num_clusters = len(cluster_sizes)

    return {
        "numTickets": int(n),
        "numClusters": int(num_clusters),
        "noiseFraction": float(noise_fraction),
        "avgSilhouette": avg_silhouette,
        "largestClusterSize": int(largest_cluster_size),
    }


def export_ticket_points(df: pd.DataFrame, coords_3d: np.ndarray, labels: np.ndarray):
    """
    Export a JS file with point coordinates + metadata for WebGL rendering.

    Structure:
    window.TICKET_POINTS = {
      points: [
        { id, x, y, z, product, category, severity, clusterId, isP1, ... },
        ...
      ]
    }
    """
    print(f"Exporting ticket_points.js â†’ {OUT_POINTS_JS}")

    points = []
    for i, row in df.iterrows():
        x, y, z = coords_3d[i].tolist()
        severity = row.get("severity", "")
        point = {
            "id": row.get("ticket_id", f"TCK-{i:06d}"),
            "x": float(x),
            "y": float(y),
            "z": float(z),
            "product": row.get("product", ""),
            "category": row.get("category", ""),
            "subcategory": row.get("subcategory", ""),
            "severity": severity,
            "customerTier": row.get("customer_tier", ""),
            "region": row.get("region", ""),
            "environment": row.get("environment", ""),
            "clusterId": int(labels[i]),
            "isP1": bool(severity == "Sev1"),
        }
        points.append(point)

    payload = {"points": points}

    with OUT_POINTS_JS.open("w", encoding="utf-8") as f:
        f.write("window.TICKET_POINTS = ")
        json.dump(payload, f)
        f.write(";\n")


def export_ticket_summary(df: pd.DataFrame, labels: np.ndarray, health: dict):
    """
    Export high-level summary metrics and distributions for Chart.js.

    Structure:
    window.TICKET_SUMMARY = {
      numTickets,
      severityDistribution,
      categoryCounts,
      productCounts,
      clusterStats: { numClusters, avgSilhouette, noiseFraction, largestClusterSize }
    }
    """
    print(f"Exporting ticket_summary.js â†’ {OUT_SUMMARY_JS}")

    n = len(df)

    # Severity distribution
    sev_counts = Counter(df["severity"].fillna("unknown"))
    severity_distribution = {
        sev: count / n for sev, count in sev_counts.items()
    }

    # Category & product counts (absolute)
    cat_counts = Counter(df["category"].fillna("unknown"))
    product_counts = Counter(df["product"].fillna("unknown"))

    # Cluster stats from model health (precomputed or inline)
    cluster_stats = {
        "numClusters": int(health.get("numClusters", 0)),
        "avgSilhouette": health.get("avgSilhouette", None),
        "noiseFraction": float(health.get("noiseFraction", 0.0)),
        "largestClusterSize": int(health.get("largestClusterSize", 0)),
    }

    summary = {
        "numTickets": int(n),
        "severityDistribution": severity_distribution,
        "categoryCounts": dict(cat_counts),
        "productCounts": dict(product_counts),
        "clusterStats": cluster_stats,
    }

    with OUT_SUMMARY_JS.open("w", encoding="utf-8") as f:
        f.write("window.TICKET_SUMMARY = ")
        json.dump(summary, f)
        f.write(";\n")


# ------------------------------------------------------------
# MAIN
# ------------------------------------------------------------

def main():
    print("==============================================")
    print(" Qognus Demo Platform â€” Export Web Artifacts")
    print("==============================================")

    df, coords_3d, labels = load_core_artifacts()
    health = maybe_load_model_health(coords_3d, labels)

    export_ticket_points(df, coords_3d, labels)
    export_ticket_summary(df, labels, health)

    print("Web artifacts exported successfully.")
    print("You can now open the web app (e.g. `cd web && python -m http.server 3000`).")
    print()


if __name__ == "__main__":
    main()


==================================================
FILE PATH: models\gridsense_timeseries\anomaly_model.py
==================================================

"""
anomaly_model.py
Qognus Demo Platform â€” ApexGrid / GridSense
-------------------------------------------

Trains and evaluates an unsupervised anomaly detection model on the
synthetic GridSense multivariate time series.

UPDATES:
- Implements Strict Temporal Splitting (Train on Past, Eval on Future).
- Auto-calibrates 'contamination' based on Training set density.
- Implements 'Soft Metrics' (Time-Tolerant Scoring).
"""

import json
import pathlib
from typing import List, Dict, Any, Tuple

import numpy as np
import pandas as pd
from sklearn.ensemble import IsolationForest

# ------------------------------------------------------------
# CONFIG
# ------------------------------------------------------------

ROOT_DIR = pathlib.Path(__file__).resolve().parent.parent.parent
RAW_DIR = ROOT_DIR / "data" / "raw"
PROC_DIR = ROOT_DIR / "data" / "processed"
WEB_DATA_DIR = ROOT_DIR / "web" / "data"

PROC_DIR.mkdir(parents=True, exist_ok=True)
WEB_DATA_DIR.mkdir(parents=True, exist_ok=True)

IN_PARQUET = RAW_DIR / "gridsense_timeseries.parquet"
OUT_PARQUET = PROC_DIR / "gridsense_timeseries_with_scores.parquet"
OUT_JS = WEB_DATA_DIR / "gridsense_timeseries_artifacts.js"

WINDOW = 12  # Rolling window size
FEATURE_COLS = ["load_mw", "voltage_kv", "current_a", "freq_hz", "oil_temp_c"]
RANDOM_SEED = 42

# ------------------------------------------------------------
# HELPERS
# ------------------------------------------------------------

def build_window_features(
    df: pd.DataFrame,
    window: int,
    feature_cols: List[str],
) -> Tuple[pd.DataFrame, np.ndarray]:
    """
    Creates rolling window features for time-series models.
    """
    df = df.copy()
    df = df.sort_values(["substation_id", "timestamp"])

    all_rows = []
    all_feats = []

    # Process per substation to avoid boundary bleeding
    for substation_id, grp in df.groupby("substation_id"):
        grp = grp.sort_values("timestamp")
        values = grp[feature_cols].to_numpy()
        labels = grp["is_anomaly"].to_numpy()
        ts = grp["timestamp"].to_numpy()
        region = grp["region"].iloc[0]

        if len(grp) < window:
            continue

        for i in range(window - 1, len(grp)):
            start = i - window + 1
            end = i + 1
            
            # Flatten window into a single feature vector
            window_vals = values[start:end, :]
            window_flat = window_vals.flatten()

            # Labeling: If >30% of the window is anomalous, label it 1
            # This reduces noise from single-point blips
            anomaly_ratio = labels[start:end].sum() / window
            window_label = 1 if anomaly_ratio > 0.3 else 0

            row = {
                "timestamp": ts[i],
                "substation_id": substation_id,
                "region": region,
                "window_label": window_label,
            }
            all_rows.append(row)
            all_feats.append(window_flat)

    window_df = pd.DataFrame(all_rows)
    X = np.array(all_feats, dtype=float)

    return window_df, X

def train_isolation_forest(X: np.ndarray, contamination: float) -> IsolationForest:
    print(f"Training IsolationForest with contamination={contamination:.3f}...")
    model = IsolationForest(
        n_estimators=200,
        contamination=contamination,
        random_state=RANDOM_SEED,
        n_jobs=-1,
    )
    model.fit(X)
    return model

def evaluate_soft_metrics(
    y_true: np.ndarray, 
    y_pred: np.ndarray, 
    tolerance: int = 2
) -> Dict[str, float]:
    """
    Calculates Precision/Recall with a time tolerance (e.g. +/- 2 steps).
    If a prediction is within 'tolerance' steps of a real anomaly, it counts as a hit.
    """
    n = len(y_true)
    if n == 0:
        return {"precision": 0.0, "recall": 0.0, "contamination": 0.0}
    
    # 1. Expand Ground Truth (Soft Targets)
    # If t is anomaly, then t-2...t+2 are valid "hit" zones
    y_true_soft = np.zeros(n, dtype=int)
    anomaly_indices = np.where(y_true == 1)[0]
    
    for idx in anomaly_indices:
        start = max(0, idx - tolerance)
        end = min(n, idx + tolerance + 1)
        y_true_soft[start:end] = 1
        
    # 2. Calculate Metrics using Soft Targets for Precision
    # (Did I predict in a valid zone?)
    tp_soft = np.sum((y_pred == 1) & (y_true_soft == 1))
    fp_soft = np.sum((y_pred == 1) & (y_true_soft == 0))
    
    precision = tp_soft / (tp_soft + fp_soft) if (tp_soft + fp_soft) > 0 else 0.0
    
    # 3. Calculate Recall (Did I catch the events?)
    # Soft Recall: If I predicted 1, and it was close to a 1, count it.
    # Inverse expansion: Expand Predictions to see if they touch Truths
    y_pred_expanded = np.zeros(n, dtype=int)
    pred_indices = np.where(y_pred == 1)[0]
    for idx in pred_indices:
        start = max(0, idx - tolerance)
        end = min(n, idx + tolerance + 1)
        y_pred_expanded[start:end] = 1
        
    tp_recall = np.sum((y_pred_expanded == 1) & (y_true == 1))
    fn_recall = np.sum((y_pred_expanded == 0) & (y_true == 1))
    
    recall = tp_recall / (tp_recall + fn_recall) if (tp_recall + fn_recall) > 0 else 0.0
    
    return {
        "precision": float(round(precision, 3)),
        "recall": float(round(recall, 3)),
        "contamination": float(round(np.mean(y_pred), 3))
    }

def export_js_artifact(
    df: pd.DataFrame,
    metrics: Dict[str, float],
    out_path: pathlib.Path,
    max_points_per_substation: int = 1000,
) -> None:
    # Ensure timestamp is ISO string
    df = df.reset_index().copy()
    df["timestamp_iso"] = df["timestamp"].dt.strftime("%Y-%m-%dT%H:%M:%SZ")

    series_records: List[Dict[str, Any]] = []

    for substation_id, grp in df.groupby("substation_id"):
        grp = grp.sort_values("timestamp")
        # Downsample
        if len(grp) > max_points_per_substation:
            idx = np.linspace(0, len(grp) - 1, max_points_per_substation).astype(int)
            grp = grp.iloc[idx]

        for _, row in grp.iterrows():
            series_records.append({
                "timestamp": row["timestamp_iso"],
                "substation_id": substation_id,
                "region": row["region"],
                "anomaly_score": float(row["anomaly_score"]),
                "predicted_anomaly": int(row["predicted_anomaly"]),
            })

    payload = {
        "summary": metrics,
        "series": series_records,
    }

    js_content = "window.GRIDSENSE_TIMESERIES = " + json.dumps(payload, indent=2) + ";\n"
    print(f"Writing JS artifact to: {out_path}")
    out_path.write_text(js_content, encoding="utf-8")

# ------------------------------------------------------------
# MAIN
# ------------------------------------------------------------

def main():
    print("===================================================")
    print(" GridSense Time Series Anomaly Model (Temporal Split) ")
    print("===================================================")

    if not IN_PARQUET.exists():
        raise FileNotFoundError(f"Input parquet not found: {IN_PARQUET}")

    df_full = pd.read_parquet(IN_PARQUET)
    df_full = df_full.sort_values(["substation_id", "timestamp"])

    print("Building rolling window features...")
    window_df, X = build_window_features(
        df_full.reset_index(), window=WINDOW, feature_cols=FEATURE_COLS
    )

    # --- 1. PREPARE TEMPORAL SPLIT ---
    # We must sort window_df and X strictly by time to ensure "Past" vs "Future"
    # Get indices that sort the DataFrame by timestamp
    sort_idxs = np.argsort(window_df["timestamp"].values)
    
    # Reorder both X and window_df using these indices
    window_df = window_df.iloc[sort_idxs].reset_index(drop=True)
    X = X[sort_idxs]
    
    # Split point: 80% Train (Past), 20% Test (Future)
    split_idx = int(len(window_df) * 0.8)
    
    X_train = X[:split_idx]
    y_train = window_df["window_label"].iloc[:split_idx]
    
    X_test = X[split_idx:]
    y_test = window_df["window_label"].iloc[split_idx:].to_numpy()

    print(f"Temporal Split: Train on first {split_idx} samples, Eval on last {len(X_test)} samples.")

    # --- 2. AUTO-CALIBRATION (Using ONLY Train Set) ---
    actual_rate = y_train.mean()
    # Add a small buffer (1.2x)
    contamination = max(0.01, min(0.15, actual_rate * 1.2))
    
    print(f"Auto-calibrated contamination: {contamination:.3f} (Train Rate: {actual_rate:.3f})")

    # --- 3. TRAIN (On Past Data Only) ---
    model = train_isolation_forest(X_train, contamination)

    # --- 4. SCORE (Full Dataset) ---
    # We score everything so the UI has a complete timeline, but metrics rely only on Test
    print("Scoring full timeline...")
    scores_raw = model.decision_function(X)
    scores = -scores_raw # Invert so high = anomaly
    
    # Normalize scores [0,1]
    scores = (scores - scores.min()) / (scores.max() - scores.min())
    
    # --- 5. PREDICT ---
    # Determine threshold based on TRAINING distribution 
    train_scores = scores[:split_idx]
    threshold = np.quantile(train_scores, 1.0 - contamination)
    
    preds = (scores >= threshold).astype(int)

    window_df["anomaly_score"] = scores
    window_df["predicted_anomaly"] = preds

    # --- 6. EVALUATE (On Future Data Only) ---
    print("\nEvaluating Performance on Held-Out Future Data (Last 20%)...")
    metrics = evaluate_soft_metrics(
        y_true=y_test, 
        y_pred=preds[split_idx:], 
        tolerance=2
    )
    print("Test Set Metrics:", metrics)

    # Merge back for export
    # Note: df_full needs to be merged with our time-sorted window_df
    df_out = df_full.reset_index().merge(
        window_df[["substation_id", "timestamp", "anomaly_score", "predicted_anomaly"]],
        on=["substation_id", "timestamp"],
        how="left"
    )
    df_out["anomaly_score"] = df_out["anomaly_score"].fillna(0.0)
    df_out["predicted_anomaly"] = df_out["predicted_anomaly"].fillna(0).astype(int)

    export_js_artifact(df_out, metrics, OUT_JS)
    print("Done.")

if __name__ == "__main__":
    main()

==================================================
FILE PATH: notebooks\01_generate_synthetic.ipynb
==================================================



==================================================
FILE PATH: notebooks\02_embeddings.ipynb
==================================================



==================================================
FILE PATH: notebooks\03_clustering.ipynb
==================================================



==================================================
FILE PATH: notebooks\04_model_health.ipynb
==================================================



==================================================
FILE PATH: notebooks\05_visualization_prototyping.ipynb
==================================================



==================================================
FILE PATH: synthetic\generate_gridsense_timeseries.py
==================================================

"""
generate_gridsense_timeseries.py
Qognus Demo Platform â€” ApexGrid Systems / GridSense
---------------------------------------------------

Generates synthetic multivariate SCADA-like time series data for GridSense
using stochastic processes and physics-based relationships.

Improvements over basic version:
1. Load Modeling: Uses an Ornstein-Uhlenbeck (mean-reverting) process 
   superimposed on a daily/weekly seasonality curve.
2. Physics: 
   - Voltage drops (sag) are proportional to Load squared (I^2*R losses).
   - Oil Temperature follows a first-order differential equation (thermal lag).
3. Realistic Anomalies:
   - "sensor_freeze": Values stick to a constant (0.0).
   - "thermal_runaway": Rapid linear climb (+40C).
   - "voltage_collapse": Severe drop (0.7x) due to reactive power events.

Output:
- data/raw/gridsense_timeseries.parquet
"""

import numpy as np
import pandas as pd
import pathlib
import datetime
from typing import List, Tuple

# ------------------------------------------------------------
# CONFIG
# ------------------------------------------------------------

DATA_DIR = pathlib.Path("data")
RAW_DIR = DATA_DIR / "raw"
RAW_DIR.mkdir(parents=True, exist_ok=True)

OUT_PARQUET = RAW_DIR / "gridsense_timeseries.parquet"

# Simulation Settings
DAYS = 14
FREQ = "5min"
DT_HOURS = 5 / 60.0  # Time step in hours

NUM_SUBSTATIONS = 16
SUBSTATIONS = [f"GS-{i:03d}" for i in range(1, NUM_SUBSTATIONS + 1)]
REGIONS = ["us-west-2", "us-east-1", "eu-central-1", "ap-southeast-1"]

RANDOM_SEED = 42

# ------------------------------------------------------------
# PHYSICS & STOCHASTIC HELPERS
# ------------------------------------------------------------

def get_seasonality(t_index: pd.DatetimeIndex) -> np.ndarray:
    """
    Returns a baseline load curve (0.0 to 1.0) based on hour of day and day of week.
    Double-hump pattern typical of residential/industrial mix.
    """
    hour = t_index.hour + t_index.minute / 60.0
    day_of_week = t_index.dayofweek
    
    # Primary daily cycle (Peak at 9AM and 7PM)
    # Using a mix of sin waves to create a non-perfect shape
    morning_peak = np.exp(-((hour - 9)**2) / 8)
    evening_peak = np.exp(-((hour - 19)**2) / 10)
    base_load = 0.3
    
    daily_shape = base_load + 0.4 * morning_peak + 0.5 * evening_peak
    
    # Weekend reduction factor (0.8x on Sat/Sun)
    weekend_factor = np.where(day_of_week >= 5, 0.8, 1.0)
    
    return daily_shape * weekend_factor

def generate_ou_process(
    length: int, 
    target_series: np.ndarray, 
    theta: float = 0.15, 
    sigma: float = 0.05, 
    rng: np.random.Generator = None
) -> np.ndarray:
    """
    Ornstein-Uhlenbeck process: dX = theta*(mu - X)*dt + sigma*dW
    Creates realistic 'wandering' around the target seasonality.
    """
    if rng is None:
        rng = np.random.default_rng()
        
    x = np.zeros(length)
    x[0] = target_series[0]
    
    noise = rng.normal(0, 1, length)
    
    for t in range(1, length):
        dx = theta * (target_series[t] - x[t-1]) * DT_HOURS + sigma * np.sqrt(DT_HOURS) * noise[t]
        x[t] = x[t-1] + dx
        
    # Clamp to physical realism (cannot have negative load)
    return np.maximum(x, 0.05)

def thermal_model(
    load_series: np.ndarray, 
    ambient_temp: np.ndarray, 
    thermal_resistance: float = 15.0, 
    tau: float = 4.0
) -> np.ndarray:
    """
    Simple thermal model for transformer oil.
    dT/dt = (1/tau) * (T_target - T_current)
    where T_target = Ambient + (Load_Factor^2 * Thermal_Resistance)
    """
    n = len(load_series)
    temp = np.zeros(n)
    temp[0] = ambient_temp[0] + (load_series[0]**2 * thermal_resistance)
    
    # Precompute decay factor for discrete step
    alpha = 1 - np.exp(-DT_HOURS / tau)
    
    for t in range(1, n):
        # I^2 R heating relationship
        heat_rise = (load_series[t]**2) * thermal_resistance
        target = ambient_temp[t] + heat_rise
        
        # Exponential smoothing (low-pass filter effect of thermal mass)
        temp[t] = temp[t-1] + alpha * (target - temp[t-1])
        
    return temp

# ------------------------------------------------------------
# SUBSTATION GENERATOR
# ------------------------------------------------------------

def generate_substation_data(
    sub_id: str, 
    region: str, 
    t_index: pd.DatetimeIndex, 
    rng: np.random.Generator
) -> pd.DataFrame:
    
    n = len(t_index)
    
    # 1. Nominal Parameters
    capacity_mw = rng.uniform(40, 120)
    nominal_voltage = 132.0 # kV
    
    # 2. Load Generation (MW)
    # Seasonal baseline + Stochastic variation
    seasonality = get_seasonality(t_index)
    load_factor = generate_ou_process(n, seasonality, theta=2.0, sigma=0.2, rng=rng)
    load_mw = load_factor * capacity_mw
    
    # 3. Ambient Temp (Daily cycle + random weather fronts)
    hour = t_index.hour + t_index.minute / 60.0
    day_temp_cycle = 20 + 5 * np.sin((hour - 10) * np.pi / 12)
    weather_fronts = generate_ou_process(n, np.zeros(n), theta=0.1, sigma=1.0, rng=rng)
    ambient_c = day_temp_cycle + weather_fronts

    # 4. Physics Derivations
    
    # Transformer Oil Temp (Lagging indicator)
    oil_temp_c = thermal_model(load_factor, ambient_c, thermal_resistance=25.0, tau=3.0)
    
    # Voltage (kV) - drops as load increases (Line Impedance)
    # V = V_nom - (Load * Impedance_Factor) + Grid_Noise
    impedance_noise = rng.normal(0, 0.05, n)
    voltage_kv = nominal_voltage * (1 - 0.02 * load_factor) + impedance_noise
    
    # Current (Amps) - P = sqrt(3) * V * I * PF
    # I = P / (sqrt(3) * V * PF)
    pf = 0.95 # Power Factor
    current_a = (load_mw * 1e3) / (np.sqrt(3) * voltage_kv * pf)
    
    # Frequency (Hz) - 50Hz or 60Hz base
    base_freq = 60.0 if "us" in region else 50.0
    freq_hz = base_freq + rng.normal(0, 0.02, n)

    df = pd.DataFrame({
        "timestamp": t_index,
        "substation_id": sub_id,
        "region": region,
        "load_mw": load_mw.round(2),
        "voltage_kv": voltage_kv.round(2),
        "current_a": current_a.round(2),
        "oil_temp_c": oil_temp_c.round(2),
        "freq_hz": freq_hz.round(3),
        "is_anomaly": 0,
        "anomaly_type": None
    })
    
    return df

# ------------------------------------------------------------
# ANOMALY INJECTION
# ------------------------------------------------------------

def inject_anomalies(df: pd.DataFrame, rng: np.random.Generator) -> pd.DataFrame:
    """
    Injects specific, severe realistic fault signatures.
    Uses .iloc to avoid off-by-one errors with inclusive slicing.
    """
    df = df.copy()
    n_points = len(df)
    
    # Indices for columns to modify
    idx_load = df.columns.get_loc("load_mw")
    idx_curr = df.columns.get_loc("current_a")
    idx_temp = df.columns.get_loc("oil_temp_c")
    idx_volt = df.columns.get_loc("voltage_kv")
    idx_anom = df.columns.get_loc("is_anomaly")
    idx_type = df.columns.get_loc("anomaly_type")

    # 1. SENSOR FREEZE (Flatline at 0.0 for obvious fault)
    if rng.random() < 0.4:
        start = rng.integers(int(n_points * 0.1), int(n_points * 0.9))
        duration = rng.integers(12, 48) # 1-4 hours
        end = min(start + duration, n_points)
        
        if end > start:
            # Flatline at 0
            df.iloc[start:end, idx_load] = 0.0
            df.iloc[start:end, idx_curr] = 0.0
            
            # Mark labels
            df.iloc[start:end, idx_anom] = 1
            df.iloc[start:end, idx_type] = "sensor_failure"

    # 2. THERMAL RUNAWAY (Cooling failure, massive climb)
    if rng.random() < 0.4:
        start = rng.integers(int(n_points * 0.1), int(n_points * 0.9))
        duration = rng.integers(24, 72) # 2-6 hours
        end = min(start + duration, n_points)
        
        if end > start:
            # Steep ramp: +40C over duration
            ramp = np.linspace(0, 40, end - start)
            
            df.iloc[start:end, idx_temp] += ramp
            
            df.iloc[start:end, idx_anom] = 1
            df.iloc[start:end, idx_type] = "thermal_runaway"

    # 3. VOLTAGE COLLAPSE (Severe Grid fault)
    if rng.random() < 0.4:
        start = rng.integers(int(n_points * 0.1), int(n_points * 0.9))
        duration = rng.integers(6, 18) # Short duration
        end = min(start + duration, n_points)
        
        if end > start:
            # 30% Voltage Drop, 50% Current Spike
            df.iloc[start:end, idx_volt] *= 0.7 
            df.iloc[start:end, idx_curr] *= 1.5 
            
            df.iloc[start:end, idx_anom] = 1
            df.iloc[start:end, idx_type] = "voltage_collapse"
        
    return df

# ------------------------------------------------------------
# MAIN
# ------------------------------------------------------------

def main():
    print("==============================================")
    print(" GridSense â€” Physics-based Data Generator (High Severity)")
    print("==============================================")
    
    rng = np.random.default_rng(RANDOM_SEED)
    
    # 1. Create Time Index
    end_time = datetime.datetime.now(datetime.timezone.utc)
    start_time = end_time - datetime.timedelta(days=DAYS)
    t_index = pd.date_range(start=start_time, end=end_time, freq=FREQ)
    
    all_subs = []
    
    # 2. Generate Data for each Substation
    for i, sub in enumerate(SUBSTATIONS):
        region = REGIONS[i % len(REGIONS)]
        print(f"Generating {sub} ({region})...")
        
        df_sub = generate_substation_data(sub, region, t_index, rng)
        df_sub = inject_anomalies(df_sub, rng)
        
        all_subs.append(df_sub)
        
    # 3. Combine and Save
    df_final = pd.concat(all_subs)
    df_final.sort_values(["substation_id", "timestamp"], inplace=True)
    
    # Save to Parquet (efficient storage)
    print(f"Saving {len(df_final)} rows to {OUT_PARQUET}...")
    df_final.to_parquet(OUT_PARQUET, index=False)
    
    print("Done. Run 'python models/gridsense_timeseries/anomaly_model.py' next to generate web artifacts.")

if __name__ == "__main__":
    main()

==================================================
FILE PATH: synthetic\generate_tickets_langchain.py
==================================================

"""
generate_tickets_langchain.py
Qognus Demo Platform â€” ApexGrid Systems
---------------------------------------

Generates fully synthetic ApexGrid support tickets using a local LLM
via Ollama, with LangChain + Pydantic structured output.

Design:
1. Generate all metadata in Python (ticket_id, product, category, etc.)
2. Ask the LLM ONLY for:
   - summary
   - description
   - topics (list of tags)
3. Use LangChain's PydanticOutputParser to enforce structure.
4. Combine meta + generated body into a final ticket object.

Output:
- data/raw/apexgrid_tickets.jsonl

Each line is a JSON object with:
  ticket_id, timestamp, product, category, subcategory, severity,
  customer_tier, region, environment, channel, summary, description, topics

Idempotent behavior:
- If the JSONL already exists, we read it, find the max ticket index
  (from ticket_id like 'TCK-000123'), and append new tickets after that.
- If we already have >= TOTAL_TICKETS, the script exits without doing work.
"""

import json
import random
import pathlib
import datetime
import time
from typing import Dict, Any, List

from tqdm import tqdm
from pydantic import BaseModel, Field
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import PydanticOutputParser
from langchain_ollama import ChatOllama


# ------------------------------------------------------------
# CONFIG
# ------------------------------------------------------------

DATA_DIR = pathlib.Path("data")
RAW_DIR = DATA_DIR / "raw"
RAW_DIR.mkdir(parents=True, exist_ok=True)

OUT_JSONL = RAW_DIR / "apexgrid_tickets.jsonl"

# Local Ollama model name; adjust to what you have pulled
# (e.g. "phi3:medium", "phi3:latest", "qwen3:8b", etc.)
OLLAMA_MODEL = "phi3:medium"

# Target total number of tickets in the JSONL
TOTAL_TICKETS = 2000

# Time window for timestamps (last X days)
TIMESTAMP_DAYS_BACK = 30

# Number of retries per ticket if LLM call fails / parses badly
MAX_RETRIES_PER_TICKET = 3


# ------------------------------------------------------------
# TAXONOMY / METADATA DEFINITIONS
# ------------------------------------------------------------

PRODUCTS = ["HelioCloud", "GridSense", "LineaOps", "VaultShield"]

CATEGORIES = {
    "authentication": [
        "MFA_failure",
        "SSO_drift",
        "oauth_token_expired",
        "unexpected_logout",
        "credential_validation_error",
    ],
    "authorization": [
        "permission_denied",
        "role_mismatch",
        "policy_conflict",
        "invalid_scope",
    ],
    "billing": [
        "invoice_discrepancy",
        "duplicate_charge",
        "credit_allocation",
        "subscription_tier_mismatch",
    ],
    "latency": [
        "p95_spike",
        "trace_delay",
        "dashboard_render_slow",
        "edge_roundtrip_high",
    ],
    "integration": [
        "SIEM_connector_failed",
        "SCADA_protocol_error",
        "PLC_driver_fault",
        "webhook_delivery_failed",
        "SSO_integration_error",
    ],
    "data_quality": [
        "missing_values",
        "schema_mismatch",
        "drift_detected",
        "timestamp_skew",
        "cardinality_explosion",
    ],
    "api_errors": [
        "429_rate_limit",
        "500_internal",
        "401_unauthorized",
        "payload_too_large",
        "malformed_request",
    ],
    "telemetry_drop": [
        "sensor_unreachable",
        "edge_offline",
        "gateway_loss",
        "ingestion_gap",
        "backlog_spike",
    ],
    "security_alerts": [
        "bruteforce_detected",
        "anomalous_login",
        "malicious_IP",
        "impossible_travel",
        "MFA_bypass_suspected",
    ],
    "observability": [
        "missing_log_stream",
        "dashboard_error",
        "trace_sampling_bug",
        "alert_deduping_failed",
    ],
    "deployment_failures": [
        "canary_failed",
        "rollout_aborted",
        "agent_upgrade_failed",
        "PLC_update_incomplete",
    ],
    "firmware": [
        "version_mismatch",
        "checksum_failure",
        "unsupported_firmware",
        "incompatible_module",
    ],
    "scaling": [
        "autoscaler_unresponsive",
        "storage_saturation",
        "pod_evicted",
        "scaling_policy_mismatch",
    ],
    "dashboard_issues": [
        "widget_failure",
        "stale_chart",
        "incorrect_units",
        "rendering_error",
    ],
}

SEVERITIES = ["Sev1", "Sev2", "Sev3", "Sev4"]
SEVERITY_WEIGHTS = [0.03, 0.12, 0.40, 0.45]  # Sev1..Sev4

CUSTOMER_TIERS = ["enterprise", "midmarket", "startup"]
CUSTOMER_TIER_WEIGHTS = [0.4, 0.4, 0.2]

REGIONS = ["us-west-2", "us-east-1", "eu-central-1", "ap-southeast-1"]
REGION_WEIGHTS = [0.3, 0.25, 0.25, 0.2]

ENVIRONMENTS = ["production", "staging", "sandbox"]
ENV_WEIGHTS = [0.7, 0.2, 0.1]

CHANNELS = ["portal", "email", "slack", "phone"]

PRODUCT_CATEGORY_MAP = {
    "HelioCloud": [
        "observability",
        "latency",
        "integration",
        "data_quality",
        "api_errors",
        "scaling",
        "dashboard_issues",
        "authentication",
        "authorization",
        "billing",
    ],
    "GridSense": [
        "telemetry_drop",
        "integration",
        "data_quality",
        "firmware",
        "deployment_failures",
        "latency",
    ],
    "LineaOps": [
        "telemetry_drop",
        "integration",
        "firmware",
        "deployment_failures",
        "data_quality",
        "latency",
    ],
    "VaultShield": [
        "authentication",
        "authorization",
        "security_alerts",
        "api_errors",
        "dashboard_issues",
        "data_quality",
    ],
}


# ------------------------------------------------------------
# Pydantic model for structured output
# ------------------------------------------------------------

class TicketBody(BaseModel):
    """
    Only the "semantic" parts that the LLM is allowed to generate.
    All other fields (product, category, severity, etc.) are injected
    by Python from metadata.
    """
    summary: str = Field(
        description="A single-sentence summary of the issue."
    )
    description: str = Field(
        description="A detailed description of the issue, 3-8 full sentences."
    )
    topics: List[str] = Field(
        description="List of 2-4 short tags related to this ticket."
    )


# ------------------------------------------------------------
# LangChain setup
# ------------------------------------------------------------

SYSTEM_PROMPT = """
You are generating fully synthetic enterprise support tickets for a fictional company named ApexGrid Systems.

Context:
- HelioCloud: SaaS observability (logs, metrics, traces, dashboards, alerts).
- GridSense: energy & utility IoT monitoring (SCADA, sensors, substations).
- LineaOps: manufacturing & robotics (PLC, lines, conveyors, robot cells).
- VaultShield: identity & security analytics (SSO, MFA, SIEM, anomalous logins).

STRICT RULES:
- Never reference real companies, people, brands, or domains.
- Use only generic references like "the customer", "their cluster", "the grid", etc.
- Align tone with professional enterprise support.
- Do NOT mention that this is synthetic or fictional.
- Do NOT include placeholders like 'lorem ipsum' or 'example.com'.
- You ONLY generate the fields: summary, description, topics.
- The final JSON format is dictated by the schema you are given.
"""

USER_TEMPLATE = """
You are generating the body of a support ticket for the ApexGrid product suite.

Here is the fixed metadata for this ticket (these values are ALREADY decided and must be respected conceptually):

ticket_id: {ticket_id}
timestamp: {timestamp}
product: {product}
category: {category}
subcategory: {subcategory}
severity: {severity}
customer_tier: {customer_tier}
region: {region}
environment: {environment}
channel: {channel}

Write:
- a concise, one-sentence SUMMARY describing the problem.
- a detailed DESCRIPTION of 3-8 full sentences:
  - mention relevant product/region/environment context naturally
  - describe what the customer observed, any diagnostics, and impact
  - keep it realistic and technically grounded for that product & category
- a TOPICS list (2-4 short tags) related to the issue.

You MUST follow the response JSON schema instructions that follow.
"""

PARSER = PydanticOutputParser(pydantic_object=TicketBody)

PROMPT = ChatPromptTemplate.from_messages([
    ("system", SYSTEM_PROMPT),
    ("user", USER_TEMPLATE),
    ("assistant", "{format_instructions}"),
])


def make_llm() -> ChatOllama:
    """
    Construct the ChatOllama LLM configured to output JSON.
    """
    return ChatOllama(
        model=OLLAMA_MODEL,
        temperature=0.4,
        format="json",  # important for structured output
    )


def make_chain():
    """
    LCEL chain: Prompt -> LLM -> Pydantic parser.
    Returns TicketBody directly on success.
    """
    llm = make_llm()
    chain = PROMPT | llm | PARSER
    return chain


# ------------------------------------------------------------
# Helpers for metadata / idempotence
# ------------------------------------------------------------

def random_ticket_id(n: int) -> str:
    return f"TCK-{n:06d}"


def random_timestamp_within_days(days_back: int) -> str:
    now = datetime.datetime.now(datetime.timezone.utc)
    delta = datetime.timedelta(days=random.uniform(0, days_back))
    ts = now - delta
    # ISO 8601 + "Z" marker
    return ts.replace(microsecond=0).isoformat().replace("+00:00", "Z")


def choose_weighted(options: List[str], weights: List[float]) -> str:
    return random.choices(options, weights=weights, k=1)[0]


def build_metadata(index: int) -> Dict[str, Any]:
    """
    Construct the full metadata dict for ticket with numeric index.
    """
    product = random.choice(PRODUCTS)
    category = random.choice(PRODUCT_CATEGORY_MAP[product])
    subcategory = random.choice(CATEGORIES[category])

    severity = choose_weighted(SEVERITIES, SEVERITY_WEIGHTS)
    customer_tier = choose_weighted(CUSTOMER_TIERS, CUSTOMER_TIER_WEIGHTS)
    region = choose_weighted(REGIONS, REGION_WEIGHTS)
    environment = choose_weighted(ENVIRONMENTS, ENV_WEIGHTS)
    channel = random.choice(CHANNELS)

    ticket_id = random_ticket_id(index)
    timestamp = random_timestamp_within_days(TIMESTAMP_DAYS_BACK)

    return {
        "ticket_id": ticket_id,
        "timestamp": timestamp,
        "product": product,
        "category": category,
        "subcategory": subcategory,
        "severity": severity,
        "customer_tier": customer_tier,
        "region": region,
        "environment": environment,
        "channel": channel,
    }


def parse_ticket_index(ticket_id: str) -> int:
    """
    Extract the numeric part from IDs of the form 'TCK-000123'.
    Returns 0 if it can't parse.
    """
    try:
        return int(ticket_id.split("-")[-1])
    except Exception:
        return 0


def load_existing_max_index(path: pathlib.Path) -> int:
    """
    If the JSONL file exists, read it and return the max ticket index.
    If it doesn't exist or is empty, return 0.
    """
    if not path.exists():
        return 0

    max_idx = 0
    with path.open("r", encoding="utf-8") as f:
        for line in f:
            line = line.strip()
            if not line:
                continue
            try:
                obj = json.loads(line)
                tid = obj.get("ticket_id", "")
                idx = parse_ticket_index(tid)
                if idx > max_idx:
                    max_idx = idx
            except Exception:
                # skip malformed lines
                continue
    return max_idx


# ------------------------------------------------------------
# Ticket generation using LangChain structured output
# ------------------------------------------------------------

def generate_ticket_body(chain, meta: Dict[str, Any]) -> TicketBody:
    """
    Invoke the LangChain chain with retries to get a TicketBody
    (summary, description, topics) for the given metadata.
    """
    for attempt in range(1, MAX_RETRIES_PER_TICKET + 1):
        try:
            result: TicketBody = chain.invoke({
                "ticket_id": meta["ticket_id"],
                "timestamp": meta["timestamp"],
                "product": meta["product"],
                "category": meta["category"],
                "subcategory": meta["subcategory"],
                "severity": meta["severity"],
                "customer_tier": meta["customer_tier"],
                "region": meta["region"],
                "environment": meta["environment"],
                "channel": meta["channel"],
                "format_instructions": PARSER.get_format_instructions(),
            })

            # Simple sanity checks:
            if len(result.summary.strip()) < 10:
                raise ValueError("Summary too short.")
            if len(result.description.split(".")) < 3:
                raise ValueError("Description too short or not enough sentences.")
            if not result.topics:
                raise ValueError("Missing topics.")

            return result

        except Exception as e:
            print(f"[{meta['ticket_id']}] Attempt {attempt} failed: {e}")
            time.sleep(1.5)

    raise RuntimeError(
        f"Failed to generate TicketBody after {MAX_RETRIES_PER_TICKET} attempts."
    )


# ------------------------------------------------------------
# MAIN
# ------------------------------------------------------------

def main():
    print("===================================================")
    print(" Qognus Demo Platform â€” LangChain Structured Tickets")
    print("===================================================")
    print(f"Ollama model: {OLLAMA_MODEL}")
    print(f"Output JSONL: {OUT_JSONL}")
    print(f"Target total tickets: {TOTAL_TICKETS}")

    # Check how many tickets we already have (if any)
    existing_max_idx = load_existing_max_index(OUT_JSONL)

    if existing_max_idx >= TOTAL_TICKETS:
        print(f"Already have {existing_max_idx} tickets (>= target). Nothing to do.")
        return

    already_have = existing_max_idx
    to_generate = TOTAL_TICKETS - already_have

    if existing_max_idx == 0:
        print("No existing tickets found. Will generate from TCK-000001.")
    else:
        print(
            f"Found existing tickets up to TCK-{existing_max_idx:06d}. "
            f"Will append {to_generate} more (to reach {TOTAL_TICKETS})."
        )

    # Build the LLM chain once
    chain = make_chain()

    generated = 0
    failed = 0

    # Open file in APPEND mode (do NOT delete/overwrite)
    with OUT_JSONL.open("a", encoding="utf-8") as f_out:
        # start from existing_max_idx + 1 up to TOTAL_TICKETS
        for idx in tqdm(
            range(existing_max_idx + 1, TOTAL_TICKETS + 1),
            desc="Generating tickets"
        ):
            meta = build_metadata(idx)
            try:
                body = generate_ticket_body(chain, meta)
            except Exception as e:
                print(f"[{meta['ticket_id']}] FAILED: {e}")
                failed += 1
                continue

            ticket = {
                **meta,
                "summary": body.summary,
                "description": body.description,
                "topics": body.topics,
            }

            f_out.write(json.dumps(ticket, ensure_ascii=False) + "\n")
            generated += 1

    print("===================================================")
    print(f"Previously existing: {already_have}")
    print(f"Newly generated:    {generated}")
    print(f"Failed:             {failed}")
    print(f"Total now (approx): {already_have + generated}")
    print(f"Written to:         {OUT_JSONL}")
    print("===================================================")


if __name__ == "__main__":
    main()


==================================================
FILE PATH: web\index.html
==================================================

<!DOCTYPE html>
<html lang="en" class="h-full">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Qognus Demo Platform</title>

  <meta
    name="description"
    content="Qognus Demo Platform â€” applied AI systems for energy, industry, cybersecurity, and critical operations."
  />

  <script src="https://cdn.tailwindcss.com"></script>
  <script>
    tailwind.config = {
      theme: {
        extend: {
          colors: {
            bgDark: "#020617",
            card: "#0f172a",
            accent: "#38bdf8",
          },
          fontFamily: {
            sans: ["Inter", "system-ui", "sans-serif"],
          },
          animation: {
            'fade-in': 'fadeIn 0.5s ease-out forwards',
          },
          keyframes: {
            fadeIn: {
              '0%': { opacity: '0', transform: 'translateY(10px)' },
              '100%': { opacity: '1', transform: 'translateY(0)' },
            }
          }
        },
      },
    };
  </script>
  <style>
    body {
      background-color: #020617;
      scroll-behavior: smooth;
    }
    .fade-in {
      animation: fadeIn 0.5s ease-out forwards;
    }
    @keyframes fadeIn {
      from { opacity: 0; transform: translateY(10px); }
      to { opacity: 1; transform: translateY(0); }
    }
    /* Scrollbar styling */
    ::-webkit-scrollbar {
      width: 8px;
    }
    ::-webkit-scrollbar-track {
      background: #0f172a;
    }
    ::-webkit-scrollbar-thumb {
      background: #334155;
      border-radius: 4px;
    }
    ::-webkit-scrollbar-thumb:hover {
      background: #475569;
    }
  </style>

  <script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
  
  <script src="./data/ticket_points.js" defer></script> 
  <script src="./data/ticket_summary.js" defer></script>
  <script src="./data/gridsense_timeseries_artifacts.js" defer></script>

  <script src="./js/charts.js" defer></script>
  <script src="./js/tabs.js" defer></script>
  <script src="./js/gridsense_card.js" defer></script>

</head>

<body class="h-full text-slate-200 font-sans selection:bg-sky-500/30 selection:text-sky-200">
  
  <header
    class="border-b border-slate-800 sticky top-0 z-50 bg-bgDark/90 backdrop-blur-md supports-[backdrop-filter]:bg-bgDark/60"
  >
    <div
      class="mx-auto px-6 py-4 flex items-center justify-between max-w-7xl"
    >
      <div class="flex items-center gap-3">
        <div class="h-6 w-6 rounded bg-gradient-to-br from-sky-400 to-indigo-500"></div>
        <div class="text-xl font-bold tracking-tight text-white">QOGNUS</div>
      </div>
      
      <nav class="flex gap-8 text-slate-400 text-sm font-medium">
        <a href="#overview" class="hover:text-white transition-colors">Overview</a>
        <a href="#cards" class="hover:text-white transition-colors">Systems</a>
        <a href="#contact" class="hover:text-white transition-colors">Contact</a>
      </nav>
    </div>
  </header>

  <section id="overview" class="max-w-7xl mx-auto px-6 pt-20 pb-20">
    <div class="max-w-3xl fade-in" style="animation-delay: 0.1s;">
        <h1
        class="text-5xl md:text-6xl font-bold tracking-tight text-white mb-8 leading-tight"
        >
        Applied AI Systems for <span class="text-transparent bg-clip-text bg-gradient-to-r from-sky-400 to-emerald-400">Critical Operations</span>.
        </h1>
        <p class="text-slate-400 text-xl leading-relaxed">
        The Qognus Demo Platform illustrates how modern enterprises apply machine
        learning across telemetry pipelines, security analytics, industrial
        automation, and large-scale anomaly detection.
        </p>
        
        <div class="mt-10 flex gap-4">
            <a href="#cards" class="px-6 py-3 rounded-full bg-slate-800 hover:bg-slate-700 text-white font-medium transition-colors border border-slate-700">
                Explore Systems
            </a>
            <a href="https://github.com/qognus" target="_blank" class="px-6 py-3 rounded-full text-slate-400 hover:text-white font-medium transition-colors flex items-center gap-2">
                View Source <span>&rarr;</span>
            </a>
        </div>
    </div>
  </section>

  <section id="cards" class="max-w-7xl mx-auto px-6 pb-32">
    <div class="flex items-center gap-4 mb-10 border-b border-slate-800 pb-4 fade-in" style="animation-delay: 0.2s;">
        <h2 class="text-2xl font-semibold text-white">Active Product Systems</h2>
        <span class="px-2.5 py-0.5 rounded-full bg-emerald-500/10 text-emerald-400 text-xs font-medium border border-emerald-500/20">All Systems Operational</span>
    </div>
    
    <div id="cardContainer" class="space-y-16">
        </div>
  </section>

  <footer id="contact" class="border-t border-slate-800 bg-slate-900/30">
    <div class="max-w-7xl mx-auto px-6 py-12 flex flex-col md:flex-row justify-between gap-10">
      
      <div class="max-w-md">
        <div class="flex items-center gap-2 mb-4">
            <div class="h-4 w-4 rounded bg-slate-600"></div>
            <span class="font-bold text-slate-200">QOGNUS</span>
        </div>
        <p class="text-slate-500 text-sm leading-relaxed">
            A fully synthetic, production-style environment for applied AI/ML demos. 
            Simulated data generated locally via Ollama and physics-based models. 
            No real customer data used.
        </p>
      </div>

      <div class="text-sm text-slate-400">
        <h4 class="font-semibold text-slate-200 mb-4">Contact</h4>
        <p class="mb-2 hover:text-white transition-colors cursor-pointer">solutions@qognus.com</p>
        <p class="mb-2 hover:text-white transition-colors cursor-pointer">github.com/qognus</p>
        <p class="mt-6 text-xs text-slate-600">
            &copy; 2025 Qognus Demo Platform. MIT License.
        </p>
      </div>
      
    </div>
  </footer>

  <script type="module">
    import { loadAllCards } from "./js/load_cards.js";

    // Load all component cards (GridSense, etc.)
    // Missing components will just log a warning but not break the page.
    loadAllCards();
  </script>
</body>
</html>

==================================================
FILE PATH: web\components\gridsense_card.html
==================================================

<section
  class="mt-10 max-w-6xl mx-auto bg-slate-900/80 border border-slate-800 rounded-3xl p-8 shadow-xl backdrop-blur-sm"
>
  <div class="flex items-start justify-between gap-6 mb-8">
    <div>
      <h3 class="text-2xl font-semibold text-slate-50">
        GridSense â€” Fleet Anomaly Detection
      </h3>
      <p class="mt-1 text-sm text-slate-300 max-w-xl">
        Real-time multivariate anomaly detection across the substation fleet.
      </p>
    </div>

    <div
      class="inline-flex items-center rounded-2xl border border-slate-700 bg-slate-950 px-4 py-2 text-xs font-medium text-slate-100 whitespace-nowrap"
    >
      <span class="mr-2 h-2 w-2 rounded-full bg-emerald-400 animate-pulse"></span>
      Live Inference
    </div>
  </div>

  <div data-tab-group="gridsense" class="flex flex-col min-h-[460px]">
    
    <div class="flex-none flex flex-wrap gap-6 border-b border-slate-800 text-sm font-medium text-slate-400 mb-6">
      <button
        type="button"
        data-tab-target="timeseries"
        class="pb-3 border-b-2 border-sky-500 text-slate-50 transition-colors"
      >
        Timeseries
      </button>
      <button
        type="button"
        data-tab-target="embedding"
        class="pb-3 border-b-2 border-transparent hover:text-slate-200 hover:border-slate-600 transition-colors"
      >
        Temporal Radar
      </button>
      <button
        type="button"
        data-tab-target="health"
        class="pb-3 border-b-2 border-transparent hover:text-slate-200 hover:border-slate-600 transition-colors"
      >
        Model Health
      </button>
    </div>

    <div data-tab-content="timeseries" class="flex-1 flex flex-col md:flex-row gap-6 min-h-0 animate-fade-in">
      
      <div class="flex-1 flex flex-col min-w-0">
        <div class="flex-1 relative rounded-2xl border border-slate-800 bg-slate-950/50 overflow-hidden w-full min-h-[300px]">
          <div class="absolute inset-0 p-2">
             <canvas data-gs-timeseries></canvas>
          </div>
        </div>
        <p id="gs-timeseries-footer" class="mt-3 text-xs text-slate-500 flex-none h-5">
          Select a substation to view telemetry.
        </p>
      </div>

      <div class="w-full md:w-40 flex flex-col flex-none border-t md:border-t-0 md:border-l border-slate-800 pt-4 md:pt-0 md:pl-4 transition-all">
        <div class="flex items-center justify-between mb-3">
          <h4 class="text-[0.65rem] font-bold text-slate-400 uppercase tracking-wider">Active Fleet</h4>
          <span class="text-[0.6rem] bg-slate-800 text-slate-400 px-1.5 py-0.5 rounded border border-slate-700">16</span>
        </div>
        
        <div 
          id="gs-substation-list" 
          class="grid grid-cols-4 sm:grid-cols-8 md:grid-cols-2 gap-1.5 overflow-y-auto overflow-x-hidden pr-1 scrollbar-thin scrollbar-thumb-slate-700 scrollbar-track-transparent content-start max-h-[350px]"
        >
          </div>
      </div>

    </div>

    <div data-tab-content="embedding" class="hidden flex-1 flex flex-col min-h-0 animate-fade-in">
      <div class="flex-1 relative rounded-2xl border border-slate-800 bg-slate-950/50 overflow-hidden w-full min-h-[300px]">
         <div class="absolute inset-0 p-2">
            <canvas data-gs-embedding></canvas>
         </div>
         <div class="absolute top-2 left-1/2 -translate-x-1/2 text-[0.6rem] font-mono text-slate-600">12:00</div>
         <div class="absolute bottom-2 left-1/2 -translate-x-1/2 text-[0.6rem] font-mono text-slate-600">06:00</div>
         <div class="absolute left-2 top-1/2 -translate-y-1/2 text-[0.6rem] font-mono text-slate-600">09:00</div>
         <div class="absolute right-2 top-1/2 -translate-y-1/2 text-[0.6rem] font-mono text-slate-600">03:00</div>
      </div>
      <div class="mt-3 flex items-start justify-between gap-4">
        <p class="text-xs text-slate-500 max-w-2xl">
          <strong>Cyclical Projection:</strong> Anomalies are plotted on a 12-hour clock face to reveal temporal patterns.
          <br><span class="opacity-70">Angle = Time of Day &nbsp;|&nbsp; Radius = Severity</span>
        </p>
        <div class="text-[0.65rem] font-mono text-slate-600 border border-slate-800 px-2 py-1 rounded bg-slate-900/50 whitespace-nowrap">
          12h Cycle
        </div>
      </div>
    </div>

    <div data-tab-content="health" class="hidden pt-2 animate-fade-in">
      <div class="grid gap-4 md:grid-cols-3 text-xs text-slate-200">
        <div class="rounded-2xl border border-slate-800 bg-slate-950/50 p-6">
          <p class="text-slate-400 text-[0.7rem] uppercase tracking-wide mb-1">Precision</p>
          <p id="gs-metric-precision" class="text-3xl font-bold text-slate-600 font-mono">--</p>
          <p class="mt-2 text-[0.7rem] text-slate-500">True Positives / All Alerts</p>
        </div>
        <div class="rounded-2xl border border-slate-800 bg-slate-950/50 p-6">
          <p class="text-slate-400 text-[0.7rem] uppercase tracking-wide mb-1">Recall</p>
          <p id="gs-metric-recall" class="text-3xl font-bold text-slate-600 font-mono">--</p>
          <p class="mt-2 text-[0.7rem] text-slate-500">Captured Incidents / Total Incidents</p>
        </div>
        <div class="rounded-2xl border border-slate-800 bg-slate-950/50 p-6">
          <p class="text-slate-400 text-[0.7rem] uppercase tracking-wide mb-1">Anomaly Rate</p>
          <p id="gs-metric-rate" class="text-3xl font-bold text-slate-600 font-mono">--</p>
          <p class="mt-2 text-[0.7rem] text-slate-500">Global fleet contamination</p>
        </div>
      </div>
    </div>

  </div>
</section>

==================================================
FILE PATH: web\js\charts.js
==================================================

function renderSparkline(ctx, scores) {
    return new Chart(ctx, {
        type: "line",
        data: {
            labels: scores.map((_, i) => i),
            datasets: [{
                data: scores,
                borderColor: "#4ade80",
                backgroundColor: "rgba(74,222,128,0.1)",
                tension: 0.25,
                pointRadius: 0,
                borderWidth: 1.5
            }]
        },
        options: {
            responsive: true,
            maintainAspectRatio: false,
            scales: { x: { display: false }, y: { display: false } },
            plugins: { legend: { display: false } }
        }
    });
}


==================================================
FILE PATH: web\js\gridsense_card.js
==================================================

// web/js/gridsense_card.js

let gsChartInstance = null;
let gsEmbedInstance = null;

const verticalLinePlugin = {
  id: 'verticalLine',
  afterDatasetsDraw(chart, args, options) {
    if (!chart.chartArea || !chart.scales.x) return;
    const { ctx, chartArea: { top, bottom, right }, scales: { x } } = chart;
    const lines = options.lines || [];
    if (lines.length === 0) return;

    ctx.save();
    ctx.lineWidth = 2;
    ctx.setLineDash([6, 4]);
    ctx.font = 'bold 10px monospace';

    lines.forEach(lineItem => {
        const { index, color, text } = lineItem;
        const meta = chart.getDatasetMeta(0);
        
        if (index < 0 || index >= meta.data.length) return;

        const xPos = x.getPixelForValue(index);
        if (xPos < chart.chartArea.left || xPos > chart.chartArea.right) return;

        ctx.beginPath();
        ctx.strokeStyle = color;
        ctx.moveTo(xPos, top);
        ctx.lineTo(xPos, bottom);
        ctx.stroke();
        
        ctx.fillStyle = color;
        const textWidth = ctx.measureText(text).width;
        const padding = 6;

        if (xPos + textWidth + padding > right) {
            ctx.textAlign = 'right';
            ctx.fillText(text, xPos - padding, top + 12);
        } else {
            ctx.textAlign = 'left';
            ctx.fillText(text, xPos + padding, top + 12);
        }
    });
    
    ctx.restore();
  }
};

window.initGridSense = function () {
  console.log('[GridSense] Initializing...');

  if (!window.GRIDSENSE_TIMESERIES) {
    console.warn('[GridSense] Artifact not found.');
    return;
  }

  const dataObj = window.GRIDSENSE_TIMESERIES;
  const series = dataObj.series;
  const metrics = dataObj.summary;

  const RECENT_WINDOW = 144; // 12 hours

  // 1. Global Time (UTC)
  let globalMaxTs = 0;
  if (series.length > 0) {
      series.forEach(d => {
          const ts = new Date(d.timestamp).getTime();
          if (ts > globalMaxTs) globalMaxTs = ts;
      });
  }
  const cutoffTime = globalMaxTs - (12 * 60 * 60 * 1000);

  // 2. Aggregate Data
  const subMap = {};

  series.forEach(d => {
    if (!subMap[d.substation_id]) {
        subMap[d.substation_id] = { 
            id: d.substation_id, 
            status: 'clean', 
            maxScore: 0,
            region: d.region,
            data: [] 
        };
    }
    const entry = subMap[d.substation_id];
    entry.data.push(d);
    
    if (d.anomaly_score > entry.maxScore) entry.maxScore = d.anomaly_score;
  });

  // 3. Determine Status
  Object.values(subMap).forEach(sub => {
      const anyAnomaly = sub.data.some(d => d.predicted_anomaly === 1);
      
      if (anyAnomaly) {
          const activeAnomaly = sub.data.some(d => {
              return d.predicted_anomaly === 1 && new Date(d.timestamp).getTime() > cutoffTime;
          });
          
          if (activeAnomaly) {
              sub.status = 'active'; 
          } else {
              sub.status = 'historic'; 
          }
      } else {
          sub.status = 'clean'; 
      }
  });

  // 4. Sort
  const subList = Object.values(subMap).sort((a, b) => {
      return a.id.localeCompare(b.id, undefined, { numeric: true, sensitivity: 'base' });
  });

  // 5. Render
  if (subList.length > 0) {
      const initialSub = subList[0];
      renderSubstationGrid(subList, initialSub.id, globalMaxTs);
      renderTimeseries(initialSub.data, initialSub.id, globalMaxTs);
      renderPseudoEmbedding(series, cutoffTime);
      updateHealthMetrics(metrics);
  }
};

function renderSubstationGrid(subList, activeId, globalMaxTs) {
    const listContainer = document.getElementById('gs-substation-list');
    if (!listContainer) return;
    
    listContainer.innerHTML = '';

    subList.forEach(sub => {
        const isActive = sub.id === activeId;
        const shortId = sub.id.split('-')[1] || sub.id;

        // CHANGED: Compact styling for narrower column (h-8, smaller text)
        let baseClass = "h-8 w-full rounded flex items-center justify-center cursor-pointer transition-all duration-200 border relative group";
        
        if (isActive) {
            baseClass += " bg-sky-500/10 border-sky-500 text-sky-100 shadow-[0_0_8px_rgba(14,165,233,0.3)]";
        } else if (sub.status === 'active') {
            baseClass += " bg-red-500/10 border-red-500/50 text-red-100 hover:bg-red-500/20";
        } else if (sub.status === 'historic') {
            baseClass += " bg-slate-800/80 border-slate-600 text-slate-300 hover:bg-slate-700 hover:text-slate-200";
        } else {
            baseClass += " bg-slate-800/40 border-slate-800 text-slate-500 hover:bg-slate-800 hover:text-slate-300";
        }

        const el = document.createElement('div');
        el.className = baseClass;
        el.title = `${sub.id} (${sub.region}) - ${sub.status}`;
        
        el.onclick = () => {
            renderSubstationGrid(subList, sub.id, globalMaxTs);
            renderTimeseries(sub.data, sub.id, globalMaxTs);
        };

        let dot = '';
        if (sub.status === 'active') {
           dot = `<span class="absolute -top-1 -right-1 flex h-2 w-2">
             <span class="animate-ping absolute inline-flex h-full w-full rounded-full bg-red-400 opacity-75"></span>
             <span class="relative inline-flex rounded-full h-2 w-2 bg-red-500"></span>
           </span>`;
        } else if (sub.status === 'historic') {
           dot = `<span class="absolute -top-1 -right-1 h-2 w-2 rounded-full bg-slate-500 ring-1 ring-slate-900"></span>`;
        }

        el.innerHTML = `
            ${dot}
            <span class="text-[0.6rem] font-mono font-bold">${shortId}</span>
        `;
        
        listContainer.appendChild(el);
    });
}

function renderTimeseries(dataPoints, subId, globalMaxTs) {
  const canvas = document.querySelector('[data-gs-timeseries]');
  if (!canvas || !window.Chart) return;
  
  if (!dataPoints || dataPoints.length === 0) {
      if (gsChartInstance) gsChartInstance.destroy();
      return;
  }

  const ctx = canvas.getContext('2d');
  const windowSize = 150;
  const cutoffTime = globalMaxTs - (12 * 60 * 60 * 1000);

  let startIndex = Math.max(0, dataPoints.length - windowSize);
  let viewModeText = "Live telemetry";

  let foundIndex = -1;
  for (let i = dataPoints.length - 1; i >= 0; i--) {
      if (dataPoints[i].predicted_anomaly === 1) {
          foundIndex = i;
          break;
      }
  }

  if (foundIndex !== -1) {
      const anomalyTime = new Date(dataPoints[foundIndex].timestamp).getTime();
      if (anomalyTime > cutoffTime) {
          const centerOffset = Math.floor(windowSize / 2);
          startIndex = Math.max(0, foundIndex - centerOffset);
          if (startIndex + windowSize > dataPoints.length) {
              startIndex = Math.max(0, dataPoints.length - windowSize);
          }
          const eventTime = new Date(dataPoints[foundIndex].timestamp);
          const dateStr = eventTime.toLocaleDateString(undefined, { month: 'short', day: 'numeric' });
          const timeStr = eventTime.toLocaleTimeString(undefined, { hour: '2-digit', minute:'2-digit' });
          viewModeText = `Incident review: <span class="text-white">${dateStr} ${timeStr}</span>`;
      }
  }

  const slice = dataPoints.slice(startIndex, startIndex + windowSize); 
  const labels = slice.map(d => {
    const date = new Date(d.timestamp);
    const month = date.toLocaleString('en-US', { month: 'short' });
    const day = date.getDate();
    const hour = String(date.getHours()).padStart(2, '0');
    const min = String(date.getMinutes()).padStart(2, '0');
    return `${month} ${day} ${hour}:${min}`;
  });
  
  const scores = slice.map(d => d.anomaly_score || 0);
  const anomalyDots = slice.map(d => d.predicted_anomaly === 1 ? (d.anomaly_score || 0) : null);

  const getPointColor = (ctx) => {
      const i = ctx.dataIndex;
      const point = slice[i];
      if (!point) return '#64748b'; 
      const pointTs = new Date(point.timestamp).getTime();
      return pointTs > cutoffTime ? '#ef4444' : '#64748b'; 
  };

  const getSegmentColor = (ctx) => {
      const i = ctx.p1DataIndex; 
      const point = slice[i];
      if (!point || point.predicted_anomaly !== 1) return '#38bdf8'; 
      const pointTs = new Date(point.timestamp).getTime();
      return pointTs > cutoffTime ? '#ef4444' : '#64748b'; 
  };

  const lineMarkers = [];
  let isAnomalyActive = false;

  slice.forEach((point, index) => {
      if (point.predicted_anomaly === 1) {
          if (!isAnomalyActive) {
              const pointTs = new Date(point.timestamp).getTime();
              const isRecent = pointTs > cutoffTime;
              
              lineMarkers.push({
                  index: index,
                  color: isRecent ? '#ef4444' : '#64748b',
                  text: isRecent ? 'ANOMALY DETECTED' : 'HISTORIC INCIDENT'
              });
              isAnomalyActive = true;
          }
      } else {
          isAnomalyActive = false;
      }
  });

  if (gsChartInstance) gsChartInstance.destroy();

  gsChartInstance = new Chart(ctx, {
    type: 'line',
    plugins: [verticalLinePlugin],
    data: {
      labels: labels,
      datasets: [
        {
          label: 'Score',
          data: scores,
          borderColor: '#38bdf8',
          borderWidth: 2,
          segment: { borderColor: getSegmentColor },
          backgroundColor: (context) => {
            const chart = context.chart;
            const {ctx, chartArea} = chart;
            if (!chartArea) return null;
            const gradient = ctx.createLinearGradient(0, chartArea.bottom, 0, chartArea.top);
            gradient.addColorStop(0, 'rgba(56, 189, 248, 0.0)');
            gradient.addColorStop(1, 'rgba(56, 189, 248, 0.2)');
            return gradient;
          },
          fill: true,
          pointRadius: 0,
          tension: 0.1,
          order: 2
        },
        {
          label: 'Anomaly',
          data: anomalyDots,
          borderColor: getPointColor,
          backgroundColor: getPointColor,
          pointRadius: 4,
          pointHoverRadius: 6,
          showLine: false,
          order: 1
        }
      ]
    },
    options: {
      responsive: true,
      maintainAspectRatio: false,
      interaction: { intersect: false, mode: 'index' },
      plugins: { 
          legend: { display: false },
          verticalLine: { lines: lineMarkers }
      },
      animation: { duration: 0 },
      scales: {
        x: { display: false },
        y: { 
          display: true, 
          min: 0, 
          suggestedMax: Math.max(...scores) * 1.1,
          grid: { color: 'rgba(30, 64, 175, 0.2)' },
          ticks: { color: '#64748b', font: {size: 10} }
        }
      }
    }
  });

  const footer = document.getElementById('gs-timeseries-footer');
  if(footer) footer.innerHTML = `${viewModeText} for <span class="font-mono text-sky-400 font-bold">${subId}</span>.`;
}

function renderPseudoEmbedding(allSeriesData, cutoffTime) {
  const canvas = document.querySelector('[data-gs-embedding]');
  if (!canvas || !window.Chart) return;
  const ctx = canvas.getContext('2d');

  const anomalies = [];
  const nominals = [];

  allSeriesData.forEach(d => {
      const date = new Date(d.timestamp);
      const dateStr = date.toLocaleString('en-US', { 
          month: 'short', 
          day: 'numeric', 
          hour: '2-digit', 
          minute: '2-digit', 
          hour12: false
      });

      const point = {
          ...d, 
          score: d.anomaly_score || 0,
          isAnom: d.predicted_anomaly === 1,
          isRecent: new Date(d.timestamp).getTime() > cutoffTime,
          fullLabel: `[${d.substation_id}] ${dateStr}`
      };
      if (point.isAnom) anomalies.push(point);
      else nominals.push(point);
  });

  const maxNominals = 800;
  const step = Math.ceil(nominals.length / maxNominals);
  const sampledNominals = [];
  for (let i = 0; i < nominals.length; i += step) {
      sampledNominals.push(nominals[i]);
  }

  const finalData = [...sampledNominals, ...anomalies].map(point => {
      const date = new Date(point.timestamp);
      
      const hours = date.getHours() % 12; 
      const mins = date.getMinutes();
      const totalMinutes = (hours * 60) + mins;
      
      const angle = (totalMinutes / 720) * (2 * Math.PI);
      
      let r;
      if (point.isAnom) {
          r = 1.0 + (point.score * 4.0); 
      } else {
          r = Math.random() * 0.8; 
      }

      return {
          x: Math.sin(angle) * r,
          y: -Math.cos(angle) * r, // Negative Cos to put 12:00 at Top
          isAnom: point.isAnom,
          isRecent: point.isRecent,
          fullLabel: point.fullLabel,
          statusLabel: point.isAnom ? (point.isRecent ? 'Active Anomaly' : 'Historic Anomaly') : 'Nominal'
      };
  });

  const normalSet = finalData.filter(d => !d.isAnom);
  const activeSet = finalData.filter(d => d.isAnom && d.isRecent);
  const historicSet = finalData.filter(d => d.isAnom && !d.isRecent);

  if (gsEmbedInstance) gsEmbedInstance.destroy();

  gsEmbedInstance = new Chart(ctx, {
    type: 'scatter',
    data: {
      datasets: [
        { 
            label: 'Nominal', 
            data: normalSet, 
            backgroundColor: '#38bdf8', 
            pointRadius: 2 
        },
        { 
            label: 'Active Anomaly', 
            data: activeSet, 
            backgroundColor: '#ef4444', 
            pointRadius: 5,
            pointHoverRadius: 7
        },
        { 
            label: 'Historic Anomaly', 
            data: historicSet, 
            backgroundColor: '#64748b', 
            pointRadius: 3 
        }
      ]
    },
    options: {
      responsive: true,
      maintainAspectRatio: false,
      scales: { x: { display: false, min: -6, max: 6 }, y: { display: false, min: -6, max: 6 } },
      plugins: { 
          legend: { display: true, labels: { color: '#94a3b8' } },
          tooltip: {
            callbacks: {
                label: (ctx) => `${ctx.raw.fullLabel} (${ctx.raw.statusLabel})`
            }
          }
      }
    }
  });
}

function updateHealthMetrics(metrics) {
  const setVal = (id, val, colorClass) => {
    const el = document.getElementById(id);
    if(el) {
        el.innerText = val;
        el.className = ''; 
        if(colorClass) el.className = `text-3xl font-bold font-mono ${colorClass}`;
    }
  };
  setVal('gs-metric-precision', (metrics.precision * 100).toFixed(1) + '%', 'text-emerald-400');
  setVal('gs-metric-recall', (metrics.recall * 100).toFixed(1) + '%', 'text-emerald-400');
  setVal('gs-metric-rate', (metrics.contamination * 100).toFixed(1) + '%', 'text-sky-400');
}

==================================================
FILE PATH: web\js\load_cards.js
==================================================

/**
 * web/js/load_cards.js
 * Loads HTML components and initializes their scripts/tabs.
 */

export async function loadAllCards() {
  const container = document.getElementById("cardContainer");
  
  if (!container) {
    console.warn("[Cards] Container element 'cardContainer' not found.");
    return;
  }

  async function loadCard(name) {
    const url = `./components/${name}_card.html`;

    try {
      const response = await fetch(url);
      if (!response.ok) throw new Error(`HTTP ${response.status}`);

      const html = await response.text();
      
      // Create a wrapper div for the card
      const wrapper = document.createElement("div");
      wrapper.classList.add("fade-in", "w-full");
      wrapper.innerHTML = html;

      container.appendChild(wrapper);
      console.log(`[Cards] Injected HTML for ${name}`);
      
      // 1. Initialize Tabs (Critical Step)
      if (window.activateTabs) {
         window.activateTabs(wrapper);
      } else {
         console.error("[Cards] window.activateTabs is missing! Check tabs.js");
      }

      // 2. Initialize Component Logic (Chart drawing)
      if (name === 'gridsense' && window.initGridSense) {
        // Short timeout ensures DOM is fully painted before Chart.js tries to grab context
        setTimeout(() => window.initGridSense(), 50);
      }
      
    } catch (err) {
      console.error(`[Cards] Failed to load card: ${name}`, err);
    }
  }

  const components = ["gridsense"];
  for (const c of components) {
    await loadCard(c);
  }
}

==================================================
FILE PATH: web\js\tabs.js
==================================================

/**
 * web/js/tabs.js
 * Handles tab switching for any component with data-tab-group attributes.
 */
window.activateTabs = function (rootElement) {
  // 1. Find all tab groups inside the newly injected element
  const groups = rootElement.querySelectorAll('[data-tab-group]');
  
  if (groups.length === 0) {
    // Fallback: check if the root itself is the group (sometimes wrapper structure varies)
    if (rootElement.hasAttribute('data-tab-group')) {
        bindGroup(rootElement);
    }
    return;
  }

  groups.forEach(group => {
    bindGroup(group);
  });
};

function bindGroup(group) {
    const buttons = group.querySelectorAll('[data-tab-target]');
    const contents = group.querySelectorAll('[data-tab-content]');

    if (buttons.length === 0) {
        console.warn("[Tabs] Found group but no buttons:", group);
        return;
    }

    console.log(`[Tabs] Activating ${buttons.length} tabs in group:`, group.getAttribute('data-tab-group'));

    buttons.forEach(btn => {
      // Remove old listeners to prevent duplicates (if re-initialized)
      const newBtn = btn.cloneNode(true);
      btn.parentNode.replaceChild(newBtn, btn);

      newBtn.addEventListener('click', () => {
        const targetName = newBtn.getAttribute('data-tab-target');

        // 1. Reset all Buttons in this group
        group.querySelectorAll('[data-tab-target]').forEach(b => {
          // Remove active styles (blue border, white text)
          b.classList.remove('border-sky-500', 'text-slate-50');
          // Add inactive styles
          b.classList.add('border-transparent', 'hover:text-slate-200');
        });

        // 2. Activate Clicked Button
        newBtn.classList.remove('border-transparent', 'hover:text-slate-200');
        newBtn.classList.add('border-sky-500', 'text-slate-50');

        // 3. Toggle Content Panels
        contents.forEach(content => {
          if (content.getAttribute('data-tab-content') === targetName) {
            content.classList.remove('hidden');
            // Small animation reset
            content.classList.remove('fade-in');
            void content.offsetWidth; // trigger reflow
            content.classList.add('fade-in');
          } else {
            content.classList.add('hidden');
          }
        });
      });
    });
}
